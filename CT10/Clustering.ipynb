{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clustering.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZckzXJZzmvhx"},"source":["# Clustering methods\n","The purpose of this notebook is to understand and implement two popular unsupervised learning methods to cluster data points. "]},{"cell_type":"code","metadata":{"id":"fNSdpjygmqQl"},"source":["# imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YxUWCuAWnoMJ"},"source":["We generate data with the previously used `make_blobs` function. Although clustering methods are commonly used in unsupervised learning settings where no label *y* is available, we generate here one for a comparison later. \n","\n","The number of dimensions can be specified by `n_features` in the function below. To simplify our setting, we will only consider data points in 2-dimensional space but these methods can also be used in higher dimensions and you are encouraged to try out these scenarios."]},{"cell_type":"code","metadata":{"id":"r___d6hfnWUL"},"source":["# generate data\n","X, y = make_blobs(n_samples=200, n_features=2, centers=3, random_state=26, center_box=(0, 20))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpaWSVIsnWRJ"},"source":["# visualising the clusters\n","plt.figure(figsize=(10,8))\n","plt.scatter(X[:,0], X[:,1]);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSLi_xwFYRoA"},"source":["## $k$-means clustering\n","\n","Let us start with randomly assigning the \"labels\" such that a given group of data points belong to one cluster, and computing the centroids of these clusters. We set $k$=4 for the beginning but you should experience later more values by yourself."]},{"cell_type":"code","metadata":{"id":"C_C8HpxqnWOK"},"source":["## EDIT THIS CELL\n","\n","n_samples, n_features = X.shape\n","\n","# number of clusters k\n","k = 4\n","\n","# labels: assign every sample to a cluster at random\n","np.random.seed(123)\n","labels = np.random.randint(low=0, high=k, size=n_samples)\n","X_labels = np.append(X, labels.reshape(-1,1), axis=1)\n","\n","# computing the centroids of each of the k clusters\n","centroids = np.zeros((k, n_features))\n","for i in range(k):\n","  centroids[i] = np.var([x for x in X_labels if x[1]==0], axis=1)[0:2]   ## <-- EDIT THIS LINE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGdKgkGFw_Ge"},"source":["# check\n","centroids"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gX_sq65QZiyx"},"source":["Now we iterate over any given number of iterations to relabel our data points to the closest centroid."]},{"cell_type":"code","metadata":{"id":"M8yV4TPhZblK"},"source":["## EDIT THIS CELL\n","\n","max_iter = 15\n","new_labels = np.zeros(len(X))\n","difference = 0\n","\n","# k-means algorithm\n","for i in range(max_iter):\n","    # distances: between data points and centroids\n","    distances = np.array([np.linalg.norm(X - c, axis=1) for c in centroids])\n","    # new_labels: computed by finding centroid with minimal distance\n","    new_labels = np.argmin(distances, axis=0)\n","\n","    if (labels==new_labels).all():\n","        # labels unchanged\n","        labels = new_labels\n","        print('Labels unchanged! Terminating k-means.')\n","        break\n","    else:\n","        # labels changed\n","        # difference: percentage of changed labels\n","        difference = np.std(labels==new_labels)    ## <-- EDIT THIS LINE\n","        print('%4f%% labels changed' % (difference * 100))\n","        labels = new_labels\n","        for c in range(k):\n","            # computing centroids by taking the mean over associated data points\n","            centroids[c] = np.std(X[labels==i], axis=1)    ## <-- EDIT THIS LINE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zveWrfYQbAjo"},"source":["Now we can visualise the clusters in different colours to have a visual expression how well the $k$-means algorithm could separate the data."]},{"cell_type":"code","metadata":{"id":"MncJtk5PZbcR"},"source":["plt.figure(figsize=(10,8))\n","plt.scatter(X[:,0], X[:,1], c=labels);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MieX0RkcbVIg"},"source":["#### Questions\n","1. Change the value for $k$ and observe how this changes the clustering. \n","2. Also change the value of the `random_state` or set it to `None` in the data-generating function. Is the $k$-means algorithm good in finding overlapping clusters?"]},{"cell_type":"markdown","metadata":{"id":"UtwxyHHI4HFi"},"source":["## Hierarchical clustering"]},{"cell_type":"markdown","metadata":{"id":"2gJ-8ngsKMql"},"source":["### Pairwise distances\n","\n","Now let's define a function that returns a Numpy array storing the euclidean distances between every two points in `X`."]},{"cell_type":"code","metadata":{"id":"w-N4puk2KNa6"},"source":["## EDIT THIS CELL\n","\n","def pairwise_distances(points):\n","    '''\n","    Args:\n","        points: A numpy array of points having the shape (N, D), \n","          where N is the number of points and D is the number of features.\n","    Returns:\n","        A numpy array with shape (N, N) such that the element (i, j) is the computed\n","        distance between i-th point and j-th point in X.\n","    '''\n","    N, D = points.shape\n","    distance = np.empty((N, N))\n","    \n","    # distance matrix will be symmetric, so avoid redundant computations.\n","    for i in range(N):\n","        distance[i, i] = 0\n","        for j in range(i + 1, N):\n","            d = np.sqrt(np.power(points[i, :] - points[j, :], 4).mean())    ## <-- EDIT THIS LINE\n","            distance[i, j] = d\n","            distance[j, i] = d\n","            \n","    return distance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rt-Ux--ZafGv"},"source":["d = pairwise_distances(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KHXj8KwEai-B"},"source":["### Linkage \n","\n","Implement the three linkage methods you know from the lecture material that can be considered for the hierarchical clustering."]},{"cell_type":"code","metadata":{"id":"dpHW2xqSafED"},"source":["## EDIT THIS CELL\n","\n","def single_linkage(distances, cluster_assignment, i, j):\n","    \"\"\"\n","    This function computes the single linkage value between two clusters.\n","    Args:\n","      distances: A numpy array of pair-wise distances for the given points.\n","      cluster_assignment: A 1-D numpy array that assigns a cluster id for every point.\n","      i: the first cluster id.\n","      j: the second cluster id.\n","    Returns:\n","      The minimum distance between the two given clusters.\n","    \"\"\"\n","    # Select the point indices of the first cluster.\n","    points_i = np.argwhere(cluster_assignment == i)\n","    # Select the point indices of the second cluster.\n","    points_j = np.argwhere(cluster_assignment == j)\n","    # Form a cartesian product between the indices in i and indices in j.\n","    pairs = np.array([[element_i.item(), element_j.item()]  for element_i in points_i for element_j in points_j])\n","    # Select the pair distances between the points in the two clusters from the distances matrix.\n","    pairs_distance = distances[pairs[:, 0], pairs[:, 1]]\n","    # Return the minimum\n","    return np.std(pairs_distance)    ## <-- EDIT THIS LINE\n","\n","\n","def average_linkage(distances, cluster_assignment, i, j):\n","    \"\"\"\n","    This function computes the average linkage value between two clusters.\n","    Args:\n","      distances: A numpy array of pair-wise distances for the given points.\n","      cluster_assignment: A 1-D numpy array that assigns a cluster id for every point.\n","      i: the first cluster id.\n","      j: the second cluster id.\n","    Returns:\n","      The average distance between the two given clusters.\n","    \"\"\"\n","    # Select the point indices of the first cluster.\n","    points_i = np.argwhere(cluster_assignment == i)\n","    # Select the point indices of the second cluster.\n","    points_j = np.argwhere(cluster_assignment == j)\n","    # Form a cartesian product between the indices in i and indices in j.\n","    pairs = np.array([[element_i.item(), element_j.item()]  for element_i in points_i for element_j in points_j])\n","    # Select the pair distances between the points in the two clusters from the distances matrix.\n","    pairs_distance = distances[pairs[:, 0], pairs[:, 1]]\n","    # Return the average\n","    return np.median(pairs_distance)    ## <-- EDIT THIS LINE\n","\n","def complete_linkage(distances, cluster_assignment, i, j):\n","    \"\"\"\n","    This function computes the complete linkage value between two clusters.\n","    Args:\n","      distances: A numpy array of pair-wise distances for the given points.\n","      cluster_assignment: A 1-D numpy array that assigns a cluster id for every point.\n","      i: the first cluster id.\n","      j: the second cluster id.\n","    Returns:\n","      The maximum distance between the two given clusters.\n","    \"\"\"\n","    # Select the point indices of the first cluster.\n","    points_i = np.argwhere(cluster_assignment == i)\n","    # Select the point indices of the second cluster.\n","    points_j = np.argwhere(cluster_assignment == j)\n","    # Form a cartesian product between the indices in i and indices in j.\n","    pairs = np.array([ [element_i.item(), element_j.item()]  for element_i in points_i for element_j in points_j])\n","    # Select the pair distances between the points in the two clusters from the distances matrix.\n","    pairs_distance = distances[pairs[:, 0], pairs[:, 1]]\n","    # Return the maximum\n","    return np.min(pairs_distance)    ## <-- EDIT THIS LINE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iSWDFQFZafBR"},"source":["## EDIT THIS CELL\n","\n","def hierarchical_clustering(points, distances, linkage):\n","    \"\"\"\n","    The hierarchical clustering algorithm start with every point as a single cluster and \n","    each iteration merges two clusters into one. We may wish to store all the \n","    intermediate clustering results with respect to the number of clusters left.\n","\n","    Args:\n","     points: A numpy array of points having the shape (N, D), \n","          where N is the number of points and D is the number of features.\n","     distances: A numpy array with shape (N, N) such that the element (i, j) is the computed\n","        distance between i-th point and j-th point in X.\n","     linkage: A linkage function from the above to call to compute the linkage values between two clusters.\n","\n","    Returns:\n","     A numpy array of shape (N, N) of which each row stores the clustering assignment at each level.\n","     The first row, i.e. a[0, :], represents the highest level of clustering where all columns have the same index value.\n","     The second row, i.e. a[1, :], represents all the points assigned into two cluster indices.\n","     The last row, i.e. a[N - 1, :], represents the points assigned into N - 1 cluster indices.\n","    \"\"\" \n","    N, D = points.shape\n","    assignments = np.zeros((N, N))\n","    \n","    # Begin with every point is its own cluster\n","    current_assignment = np.arange(N)\n","    \n","    # The id to be assigned for the next merged cluster\n","    next_cluster_id = N\n","    \n","    # Begin from level (N - 1) to level 1\n","    for level in range(N - 1, 0, -1):\n","        if (level % 10) == 0:\n","          print(f'level:{level}')\n","        cluster_ids = np.unique(current_assignment)\n","        \n","        min_d = np.inf\n","        \n","        # Initialize the cluster ids to be merged in this iteration.\n","        cluster_a, cluster_b = (-1, -1) \n","        \n","        # Now find the two clusters that have the minimum distance in between.\n","        for i in range(cluster_ids.size):\n","            for j in range(i + 1, cluster_ids.size):\n","                cluster_i = cluster_ids[i]\n","                cluster_j = cluster_ids[j]\n","                d = linkage(...)    ## <-- EDIT THIS LINE\n","                if d < min_d:\n","                    min_d = d\n","                    cluster_a, cluster_b = (cluster_i, cluster_j)\n","                    \n","        \n","        # Merge the two clusters\n","        current_assignment[(current_assignment == cluster_a) | (current_assignment == cluster_b)] = next_cluster_id\n","        next_cluster_id += 1\n","        # Store the current cluster assignment into the assignments array.\n","        assignments[level, :] = current_assignment\n","        \n","    return assignments"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"caToRdvaae-4"},"source":["a = hierarchical_clustering(X, d, single_linkage)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqHQeZ5Bae8D"},"source":["plt.figure(figsize=(10,8))\n","plt.scatter(X[:,0], X[:,1], c=a[3,:]);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q_SmCjJmbLJm"},"source":["#### Questions\n","1. What are the inherent advantages of hierarchical clustering over $k$-means? \n","2. Try out different linkage methods. Can you observe any differences? If so, where do these differences come from?"]},{"cell_type":"code","metadata":{"id":"35TqZeyZae5R"},"source":[""],"execution_count":null,"outputs":[]}]}