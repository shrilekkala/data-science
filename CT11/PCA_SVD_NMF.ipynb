{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emCvo0kWU6dN"
   },
   "source": [
    "\n",
    "# Projection Methods\n",
    "\n",
    "\n",
    "The purpose of this notebook is to understand and implement different projection methods. In particular we will look at:\n",
    "\n",
    "\n",
    "\n",
    "1.   Principle component analysis (PCA)\n",
    "2.   Singular value decomposition (SVD)\n",
    "3.   Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "\n",
    "These methods are forms of matrix decompositions, whereby the matrix can be factorised into the product of matrices. **Matrix factorisation can reduce a matrix into constituent parts that make it easier to calculate more complex matrix operations.** These methods form the foundation of linear algebra in computers, even for basic operations such as solving systems of linear equations, calculating the inverse, and calculating the determinant of a matrix. However, **we will predominantly use these methods for dimensionality reduction, i.e. reducing the number of features for purposes of supervised, unsupervised learning and visualisation**.\n",
    "\n",
    "All of these methods can be implemented using python libraries such as sklearn, however, here we will choose to implement these methods manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYwtD2z9U4Oh"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEDuVSbJZccc"
   },
   "source": [
    "Lets first load some data. We will use the mnist dataset again for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyKtEWzcZcD6"
   },
   "outputs": [],
   "source": [
    "# loading images\n",
    "images, labels = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 25764,
     "status": "ok",
     "timestamp": 1607853702539,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "dNg8MaHLZ6Ur",
    "outputId": "95a5fa13-f4d8-420b-b779-ece6bc369ff2"
   },
   "outputs": [],
   "source": [
    "# plot the first image\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(images[:1].reshape(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oN7fm0KZZ-BS"
   },
   "source": [
    "Lets now preprocess the data together. When you are working on real world problems, you will need to do all these steps by yourself.\n",
    "\n",
    "**The steps below ensure that our images will have zero mean and one variance.** These preprocessing\n",
    "steps are also known as [Data Normalization or Feature Scaling](https://en.wikipedia.org/wiki/Feature_scaling).\n",
    "\n",
    "\n",
    "The preprocessing steps we will do are:\n",
    "1. Convert unsigned interger 8 (uint8) encoding of pixels to a floating point number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-6bcoKlZ6Xm"
   },
   "outputs": [],
   "source": [
    "# Some preprocessing of the data\n",
    "\n",
    "# we take the first 1000 images\n",
    "n_datapoints = 1000\n",
    "\n",
    "# reshaping and taking a slice of the data\n",
    "X = (images.reshape(-1, 28 * 28)[:n_datapoints]) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38-ReaBUajXs"
   },
   "source": [
    "The next two steps of the preprocessing is:\n",
    "\n",
    "2. Subtract from each image the mean $\\boldsymbol \\mu$.\n",
    "3. Scale each dimension of each image by $\\frac{1}{\\sigma}$ where $\\sigma$ is the stardard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fBMihh7asJe"
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    std_filled = std.copy()\n",
    "    std_filled[std==0] = 1.\n",
    "    Xbar = ((X-mu)/std_filled)\n",
    "    return Xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmDhw0tMajgj"
   },
   "outputs": [],
   "source": [
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSCbEI9AZBYs"
   },
   "source": [
    "# 1. PCA\n",
    "\n",
    "Now we will implement PCA. Before we do that, let's pause for a moment and\n",
    "think about the steps for performing PCA. Assume that we are performing PCA on\n",
    "some dataset $\\boldsymbol X$ for $k$ principal components. \n",
    "We then need to perform the following steps, which we break into parts:\n",
    "\n",
    "1. Compute the covariance matrix  $\\mathbf C = \\mathbf X^\\top \\mathbf X/(n-1)$\n",
    "1. Find eigenvalues and corresponding eigenvectors for the covariance matrix, $\\mathbf C = \\mathbf V \\mathbf L \\mathbf V^\\top,$\n",
    "3.  Sort by the largest eigenvalues and the corresponding eigenvectors.\n",
    "4.  Compute the projection onto the spaced spanned by the top $n$ eigenvectors.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4YJZmlzdN4N"
   },
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION - DONE\n",
    "\n",
    "from scipy.sparse import linalg\n",
    "\n",
    "def pca_function(X,k):\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # create covariance matrix S\n",
    "    C = X.T @ X / (n-1)\n",
    "\n",
    "    # compute eigenvalues and eigenvectors using the eigsh scipy function\n",
    "    ## LM means find the k largest in magnitude (eigenvalues)\n",
    "    eigenvalues, eigenvectors = linalg.eigsh(C, k, which=\"LM\", return_eigenvectors=True) \n",
    "\n",
    "    # sorting the eigenvectors and eigenvalues from largest to smallest eigenvalue\n",
    "    sorted_index = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_index]\n",
    "    eigenvectors = eigenvectors[:,sorted_index ]\n",
    "\n",
    "    # transform our data\n",
    "    X_pca = X.dot(eigenvectors)\n",
    "\n",
    "    return X_pca, eigenvectors, eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idrpUm4gmsU7"
   },
   "source": [
    "Lets now apply our PCA function to create three principle components, k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNPDJWWci-Uw"
   },
   "outputs": [],
   "source": [
    "# choose our number of principle components\n",
    "k = 3\n",
    "\n",
    "# compute the projection, the eigenvector and eigenvalues from our function\n",
    "X_pca, eigenvectors, eigenvalues = pca_function(X,k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X_pca.shape)\n",
    "print(eigenvectors.shape)\n",
    "print(eigenvalues.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c28wTKEsw3RF"
   },
   "source": [
    "Lets now plot our original images projected onto a 2d plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 26078,
     "status": "ok",
     "timestamp": 1607853702876,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "ShMbH704jtSS",
    "outputId": "2eb5827e-4f14-4711-cea0-e75ff649164c"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_pca[:,0],X_pca[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO9p4-NVvru6"
   },
   "source": [
    "Lets now compute the variance in each principle component. Then print the variance for the first three principle components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26073,
     "status": "ok",
     "timestamp": 1607853702879,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "RHS0IRTjkA6q",
    "outputId": "782f8a3f-190d-4d0d-f82f-34b296509355"
   },
   "outputs": [],
   "source": [
    "## EDIT HERE - DONE\n",
    "\n",
    "explained_variances = (eigenvalues / eigenvalues.sum())\n",
    "print('The explained variance for the first three principle components is: {}'.format(explained_variances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsEEuifSkBeS"
   },
   "source": [
    "Lets now repeat this using the sklearn PCA function to confirm we have done it correctly.\n",
    "\n",
    "**The visual might be inversed but the relative distances between points is consistent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 26783,
     "status": "ok",
     "timestamp": 1607853703595,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "nY67csTchXfM",
    "outputId": "9d16ffc9-eded-4e26-c6c8-bccfdd539d21"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=k, svd_solver='full')\n",
    "X_sk_pca = pca.fit_transform(X)\n",
    "plt.scatter(X_sk_pca[:,0],X_sk_pca[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26779,
     "status": "ok",
     "timestamp": 1607853703596,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "Delbzcqw5sxi",
    "outputId": "07e35867-4c4b-4d5b-ecc2-c4401a14382a"
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n",
    "## pca.explained_variance_ratio_/np.sum(pca.explained_variance_ratio_) gives the same explained variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPaGe1PW7wyn"
   },
   "source": [
    "Lets now try and understand what these different principle components represent for a given image!\n",
    "\n",
    "Lets plot the first 25 eigenvectors reshaped into their original image size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 865
    },
    "executionInfo": {
     "elapsed": 30721,
     "status": "ok",
     "timestamp": 1607853707543,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "x3mKFAcr72gz",
    "outputId": "21fa2f0d-9068-4fd9-be6e-e257b800196e"
   },
   "outputs": [],
   "source": [
    "## EDIT HERE - DONE\n",
    "\n",
    "k = 25\n",
    "X_pca, eigenvectors, eigenvalues = pca_function(X,k)\n",
    "\n",
    "nrow = 5; ncol = 5;\n",
    "fig, axs = plt.subplots(nrows=nrow, ncols=ncol,figsize=(15,15))\n",
    "\n",
    "for i,ax in enumerate(axs.reshape(-1)): \n",
    "    ax.imshow(eigenvectors[:,i].reshape([28,28]))\n",
    "    ax.set_ylabel(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIBYgodIth8o"
   },
   "source": [
    "# 2. SVD\n",
    "\n",
    "We now want to perform a singular value decomposition of our data,\n",
    "\n",
    "<center>\n",
    "$\n",
    "\\mathbf X = \\mathbf U \\mathbf S \\mathbf V^\\top,\n",
    "$\n",
    "</center>\n",
    "\n",
    "\n",
    "where $\\mathbf U$ is the unitary matrix, $\\mathbf S$ is the diagonal matrix of singular values $s_i$, and  $\\mathbf V$ are the principal directions/axes.\n",
    "\n",
    "We can see a link with PCA at this moment; we can compute the covariance matrix as,\n",
    "\n",
    "<center>\n",
    "$\n",
    "\\mathbf C = \\mathbf V \\mathbf S \\mathbf U^\\top \\mathbf U \\mathbf S \\mathbf V^\\top /(n-1) = \\mathbf V \\frac{\\mathbf S^2}{n-1}\\mathbf V^\\top,\n",
    "$\n",
    "</center>\n",
    "\n",
    "\n",
    "which confirms that $\\mathbf V$ are the principal directions/axes, and the columns of $\\mathbf U \\mathbf S$ are our principal components. The singular values $\\lambda_i = s_i^2/(n-1)$ correspond to the eigenvalues.\n",
    "\n",
    "\n",
    "We can define our principal components explicitly as,\n",
    "\n",
    "<center>\n",
    "$\\mathbf X \\mathbf V = \\mathbf U \\mathbf S \\mathbf V^\\top \\mathbf V = \\mathbf U \\mathbf S$\n",
    "</center>\n",
    "\n",
    "\n",
    "To perform the SVD we will just use the [svd](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.svd.html) function from scipy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeKl9PkwU4T8"
   },
   "outputs": [],
   "source": [
    "## EDIT HERE - DONE\n",
    "\n",
    "# lets use the SVD algorithm from scipy\n",
    "U, s, V = sc.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-fk_jSNQhBO"
   },
   "source": [
    "Now compute the projection of the features X by dot product with the principle axes V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 31061,
     "status": "ok",
     "timestamp": 1607853707892,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "gV_7gOUJ0NRZ",
    "outputId": "d02d4ee6-d279-4783-f2f8-7fa437d32794"
   },
   "outputs": [],
   "source": [
    "## EDIT HERE - DONE\n",
    "\n",
    "# given our decomposition, create the projection of our features X by dot product with the principle axes V\n",
    "\n",
    "X_svd = X.dot(V.T)\n",
    "plt.scatter(X_svd[:,0], X_svd[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ci_qOTOxtjz"
   },
   "source": [
    "To confirm that we have performed the projection correctly, lets use the [TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD) function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 31348,
     "status": "ok",
     "timestamp": 1607853708184,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "tsb1WhQC0Trd",
    "outputId": "ccb5fe46-d724-417d-e031-3db0d90e589a"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "X_svd = svd.fit_transform(X)\n",
    "plt.scatter(X_svd[:,0],X_svd[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPuIZbEP7E5P"
   },
   "source": [
    "# 3. NMF\n",
    "\n",
    "Now we will look at non-negative matrix factorisation. NMF is a matrix factorization method where we **constrain the matrices to be nonnegative**. PCA previously produced factors that could be both positive and negative. \n",
    "\n",
    "NMF factors our n-by-m feature matrix $\\mathbf X$ into nonnegative factors $\\mathbf W$ (n-by-k) and $\\mathbf H$ (k-by-m). The factorization is not exact. $\\mathbf W\\mathbf H$ is a lower-rank approximation to $\\mathbf A$. The factors $\\mathbf W$ and $\\mathbf H$ minimize the root mean square residual D between $\\mathbf X$ and $\\mathbf W\\mathbf H$.\n",
    "\n",
    "Note that for non-negative matrix factorisation we require the input matrix to be non-negative. Therefore, we must normalise between 0 and 1 instead of standard normalise which we used for PCA and SVD.\n",
    "\n",
    "\n",
    "Here we will use Lee and Seung's multiplicative update rule. \n",
    "\n",
    "<center>\n",
    "$\n",
    "\\mathbf{H}_{[i,j]}^{n+1}\\leftarrow \\mathbf{H}_{[i,j]}^{n} \n",
    "\\frac{(( \\mathbf{W}^n)^T \\mathbf{V})_{[i,j]}}{((\\mathbf{W}^n)^T \\mathbf{W}^n \\mathbf{H}^n)_{[i,j]}}\n",
    "$\n",
    "</center>\n",
    "\n",
    "and\n",
    "\n",
    "<center>\n",
    "$\n",
    "\\mathbf{W}_{[i,j]}^{n+1}\\leftarrow \\mathbf{W}_{[i,j]}^{n} \n",
    "\\frac{(\\mathbf{V}(\\mathbf{H}^{n+1})^T)_{[i,j]}}{(\\mathbf{W}^n \\mathbf{H}^{n+1} (\\mathbf{H}^{n+1})^T)_{[i,j]}}\n",
    "$\n",
    "</center>\n",
    "\n",
    "until $\\mathbf{W}$ and $\\mathbf{H}$ are stable. It is important to note that updates are done on an element by element basis and not matrix multiplication.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMXf2zJULcKk"
   },
   "outputs": [],
   "source": [
    "## EDIT THIS FUNCTION - DONE\n",
    "\n",
    "# normalise min max to 0-1\n",
    "def normalize_nmf(X):\n",
    "    X_norm = (X - np.min(X)) / (np.max(X) - np.min(X))\n",
    "    return X_norm\n",
    "\n",
    "X = normalize_nmf(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vQKXwBBvfvN"
   },
   "source": [
    "Since we are performing an optimisation, we will need to have some placeholder matrices with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAb3HqswLcTW"
   },
   "outputs": [],
   "source": [
    "# choosing the number of dimensions on which to project\n",
    "k = 2\n",
    "\n",
    "# setting the random seed (just so everyone gets the same results...)\n",
    "np.random.seed(0)\n",
    "\n",
    "# m x n components matrix, usually interpreted as the basis set \n",
    "W = np.random.rand(X.shape[0], k)\n",
    "\n",
    "# n x n matrix interpreted as the coefficients\n",
    "H = np.random.rand(k, X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mi0Kp6lqvpUh"
   },
   "source": [
    "An optimisation needs a cost function, which we define as the root mean square residuals between our original feature matrix X and our the product of our two factored matrices.\n",
    "\n",
    "Our chi2 can be defined as,\n",
    "\n",
    "<center>\n",
    "$\n",
    "\\chi^2 = \\frac{e^⊤ (X \\odot d \\odot d) e}{ n}\n",
    "$\n",
    "</center>\n",
    "\n",
    "where d is the difference $d = X - WH$, and e is a vector of all ones where $e^⊤ K e$ simply performs a sum over all elements in K, and $n$ is the total number of elements in $X$.\n",
    "\n",
    "The symbol $\\odot$ is the element wise product (Hadamard product).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxjXmQzwLcWx"
   },
   "outputs": [],
   "source": [
    "## EDIT HERE - DONE\n",
    "\n",
    "# lets define a cost matrix to compare the difference\n",
    "def cost(X,W,H):\n",
    "\n",
    "    # compute the difference between X and the dot product of W and H\n",
    "    diff = X - np.dot(W, H)\n",
    "\n",
    "    # element wise multiplication of V*diff and diff, all divided by the total number of non zero elements in V\n",
    "    chi2 = ((X*diff) * diff).sum() / (X.shape[0]*X.shape[1])\n",
    "\n",
    "    return chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlvSAxgShslE"
   },
   "source": [
    "Now implement Lee and Seung's multiplicative update rule. First implement the update on H and then the update on W. Finally compute the chi2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFICp8R5bmzW"
   },
   "outputs": [],
   "source": [
    "## EDIT HERE - DONE\n",
    "\n",
    "# set the number of iterations\n",
    "n_iters = 200\n",
    "\n",
    "# empty list for chi2 \n",
    "chi2 = []\n",
    "\n",
    "# loop over the n iterations\n",
    "for i in range(n_iters):\n",
    "\n",
    "    # compute the update on H\n",
    "    H = H * ((W.T.dot(X)) / W.T.dot(W.dot(H)))\n",
    "\n",
    "    # compute the update on W\n",
    "    W = W * ((X.dot(H.T)) / (W.dot(H.dot(H.T))))\n",
    "\n",
    "    # compute the chi2 and append to list\n",
    "    chi2.append(cost(X,W,H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIfQhL7rv6pr"
   },
   "source": [
    "We should next check to confirm that we have converged to a solution by plotting the chi2 of our cost function over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 36270,
     "status": "ok",
     "timestamp": 1607853713123,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "o6tLWQaYSWW6",
    "outputId": "d2eaeeac-8e96-4654-9c74-91c9aa3ca362"
   },
   "outputs": [],
   "source": [
    "# plotting the cost\n",
    "plt.plot(chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHXIZARZTUL9"
   },
   "source": [
    "We can see above that we have converged in optimising our NMF according to the chi2 cost function.\n",
    "\n",
    "Lets now plot our components using the matrix W.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 36587,
     "status": "ok",
     "timestamp": 1607853713444,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "OoqDv0MHTiLt",
    "outputId": "6bf13849-b897-413b-bb5f-0144f6d75bef"
   },
   "outputs": [],
   "source": [
    "plt.scatter(W[:,0],W[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_P12bw8O2yv"
   },
   "source": [
    "\n",
    "We can now do this using sklearn and check that our methodology was implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 37278,
     "status": "ok",
     "timestamp": 1607853714139,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "g-avVrXMMsl0",
    "outputId": "0e031bac-0d52-4bbb-aec3-af0f849cc6fa"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=k, init='random', random_state=0)\n",
    "W = nmf.fit_transform(X)\n",
    "plt.scatter(W[:,0],W[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWVGyTqrwK-C"
   },
   "source": [
    "Finally, we can have a look at the difference components resulting from our NMF and see what information they might contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 37594,
     "status": "ok",
     "timestamp": 1607853714462,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "i4SYBO6MMwRB",
    "outputId": "74796e70-18ca-4247-8888-2aa0ef44d465"
   },
   "outputs": [],
   "source": [
    "nrow = 1; ncol = 2;\n",
    "fig, axs = plt.subplots(nrows=nrow, ncols=ncol)\n",
    "\n",
    "for i,ax in enumerate(axs.reshape(-1)): \n",
    "    ax.imshow(nmf.components_[i,:].reshape([28,28]))\n",
    "    ax.set_ylabel(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQuOcAlZdsEp"
   },
   "source": [
    "Alternative code for computing optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 44648,
     "status": "ok",
     "timestamp": 1607853721521,
     "user": {
      "displayName": "robert peach",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GileV_LF-OjLqorb5DsEdqR4iNxzZhX8mH4agmI=s64",
      "userId": "06754477876396858374"
     },
     "user_tz": -60
    },
    "id": "yooh4ZiLdpXr",
    "outputId": "b8d470de-9703-44cd-9138-6f68b2d14160"
   },
   "outputs": [],
   "source": [
    "# lets define a cost matrix to compare the difference\n",
    "def cost(X,W,H,V):\n",
    "\n",
    "    # compute the difference between X and the dot product of W and H\n",
    "    diff = X - np.dot(W, H)\n",
    "\n",
    "    # element wise multiplication of V*diff and V, all divided by the total number of non zero elements in V\n",
    "    chi2 = ((V*diff) * diff).sum() / np.count_nonzero(V)\n",
    "\n",
    "    return chi2\n",
    "\n",
    "\n",
    "# choosing the number of dimensions on which to project\n",
    "k = 2\n",
    "\n",
    "# setting the random seed (just so everyone gets the same results...)\n",
    "np.random.seed(0)\n",
    "\n",
    "# m x n components matrix, usually interpreted as the basis set \n",
    "W = np.random.rand(X.shape[0], k)\n",
    "\n",
    "# n x n matrix interpreted as the coefficients\n",
    "H = np.random.rand(k, X.shape[1])\n",
    "\n",
    "# m x n matrix, the weight, (usually) the inverse variance\n",
    "V = np.ones(X.shape)\n",
    "VT = V.T\n",
    "\n",
    "# set the number of iterations\n",
    "n_iters = 200\n",
    "\n",
    "# matrix multiplication between V and X\n",
    "XV = np.multiply(V, X)\n",
    "\n",
    "# matrix multiplication between tranpose V and tranpose X\n",
    "XVT = np.multiply(VT, X.T)\n",
    "chi2 = []\n",
    "\n",
    "# looping over n iterations\n",
    "for i in range(n_iters):\n",
    "\n",
    "    H_up = np.dot(XVT, W)\n",
    "    WHVT = np.multiply(VT, np.dot(W, H).T)\n",
    "    H_down = np.dot(WHVT, W)\n",
    "    H = H*H_up.T/H_down.T\n",
    "\n",
    "\n",
    "    W_up = np.dot(XV, H.T)\n",
    "    WHV = np.multiply(V, np.dot(W, H))\n",
    "    W_down = np.dot(WHV, H.T)\n",
    "    W = W*W_up/W_down\n",
    "\n",
    "    chi2.append(cost(X,W,H,V))\n",
    "\n",
    "plt.plot(chi2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPvv4xU1DlgKPd/XNUxXXiH",
   "collapsed_sections": [],
   "name": "PCA_SVD_NMF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
