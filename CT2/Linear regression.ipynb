{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear regression.ipynb","provenance":[{"file_id":"https://github.com/probml/pyprobml/blob/master/notebooks/intro/linreg.ipynb","timestamp":1600853686600}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VzLYzhtDt9Gl"},"source":["# Linear regression\n","\n","*Partly adapted from [Deisenroth, Faisal, Ong (2020)](https://mml-book.github.io/).*\n","\n","The purpose of this notebook is to practice implementing some linear algebra (equations provided) and to explore some properties of linear regression.\n","\n","We will solely rely on the Python packages numpy and matplotlib, and you are not allowed to use any package that has a complete linear regression framework implemented (e.g., scikit-learn).\n"]},{"cell_type":"code","metadata":{"id":"bgdhMY0gu00z","executionInfo":{"status":"ok","timestamp":1609766220138,"user_tz":0,"elapsed":712,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wrdWPxiMrRvu"},"source":["We consider a linear regression problem of the form\n","$$\n","y = \\boldsymbol x^T\\boldsymbol\\beta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n","$$\n","where $\\boldsymbol x\\in\\mathbb{R}^D$ are inputs and $y\\in\\mathbb{R}$ are noisy observations. The parameter vector $\\boldsymbol\\beta\\in\\mathbb{R}^D$ parametrizes the function.\n","\n","We assume we have a training set $(\\boldsymbol x_n, y_n)$, $n=1,\\ldots, N$. We summarize the sets of training inputs in $\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}$ and corresponding training targets $\\mathcal Y = \\{y_1, \\ldots, y_N\\}$, respectively.\n","\n","In this tutorial, we are interested in finding parameters $\\boldsymbol\\beta$ that map the inputs well to the ouputs.\n","\n","From our lectures, we know that the parameters $\\boldsymbol\\beta$ found by the following equation are optimal:\n","$$\n","\\underset{\\boldsymbol\\beta}{\\text{min}} \\| \\mathcal Y - \\mathcal X \\boldsymbol\\beta \\|^2 = \\underset{\\boldsymbol\\beta}{\\text{min}} \\ \\text{L}_{\\text{LS}} (\\boldsymbol\\beta)\n","$$\n","where $\\text{L}_{\\text{LS}}$ is the (ordinary) least squares loss function. The solution is\n","$$\n","\\boldsymbol\\beta^{*} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y \\ \\in\\mathbb{R}^D\\,,\n","$$\n","where \n","$$\n","\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n","$$\n","\n","We will start with a simple training set, that we define by ourselves."]},{"cell_type":"code","metadata":{"id":"RvAyeBZort-_","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1609766231275,"user_tz":0,"elapsed":990,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"f92737b3-d48a-45af-bcc8-66bb14e0d836"},"source":["# Define training set\n","X = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\n","y = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n","\n","# Plot the training set\n","plt.figure()\n","plt.plot(X, y, '+', markersize=10)\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\");"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"FXcgW-H4sJJ1"},"source":["## 1. Maximum likelihood\n","We will start with what you have gotten to know as the statistical interpretation of linear regression, i.e., the maximum likelihood estimation of the parameters $\\boldsymbol\\beta$. In maximum likelihood estimation, we find the parameters $\\boldsymbol\\beta^{\\mathrm{ML}}$ that maximize the likelihood\n","$$\n","p(\\mathcal Y | \\mathcal X, \\boldsymbol\\beta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\beta)\\,.\n","$$\n","From the lecture we know that the maximum likelihood estimator is given by\n","$$\n","\\boldsymbol\\beta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y \\, .\n","$$\n","\n","Let us compute the maximum likelihood estimate for the given training set."]},{"cell_type":"code","metadata":{"id":"517e-cosr6Om","executionInfo":{"status":"ok","timestamp":1609766320611,"user_tz":0,"elapsed":739,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["## EDIT THIS FUNCTION\n","def max_lik_estimate(X, y):\n","    \n","    # X: N x D matrix of training inputs\n","    # y: N x 1 vector of training targets/observations\n","    # returns: maximum likelihood parameters (D x 1)\n","    \n","    N, D = X.shape\n","    beta_ml = np.zeros((D,1)) ## <-- EDIT THIS LINE\n","    return beta_ml"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZr2lngBuAo6","executionInfo":{"status":"ok","timestamp":1609766323801,"user_tz":0,"elapsed":542,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["# get maximum likelihood estimate\n","beta_ml = max_lik_estimate(X,y)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xjUdOCVht9aW"},"source":["Now, make a prediction using the maximum likelihood estimate that we just found."]},{"cell_type":"code","metadata":{"id":"MBErjQspt93g"},"source":["## EDIT THIS FUNCTION\n","def predict_with_estimate(X_test, beta):\n","    \n","    # X_test: K x D matrix of test inputs\n","    # beta: D x 1 vector of parameters\n","    # returns: prediction of f(X_test); K x 1 vector\n","    \n","    prediction = X_test ## <-- EDIT THIS LINE\n","    \n","    return prediction "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-jS04fBugPO"},"source":["Let's see whether we got something useful:"]},{"cell_type":"code","metadata":{"id":"LJVfgk7gujGu"},"source":["# define a test set\n","X_test = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n","\n","# predict the function values at the test points using the maximum likelihood estimator\n","ml_prediction = predict_with_estimate(X_test, beta_ml)\n","\n","# plot\n","plt.figure()\n","plt.plot(X, y, '+', markersize=10)\n","plt.plot(X_test, ml_prediction)\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-DNSk4tXvEhO"},"source":["#### Questions\n","1. Does the solution above look reasonable?\n","2. Play around with different values of $\\beta$. How do the corresponding functions change?\n","3. Modify the training targets $\\mathcal Y$ and re-run your computation. What changes?\n","\n","Let us now look at a different training set, where we add 2.0 to every $y$-value, and compute the maximum likelihood estimate."]},{"cell_type":"code","metadata":{"id":"MXsZCHgAvE-6"},"source":["ynew = y + 2.0\n","\n","plt.figure()\n","plt.plot(X, ynew, '+', markersize=10)\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\");"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A8qYt3V_vPlG"},"source":["# get maximum likelihood estimate\n","beta_ml = max_lik_estimate(X, ynew)\n","print(beta_ml)\n","\n","# define a test set\n","X_test = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n","\n","# predict the function values at the test points using the maximum likelihood estimator\n","ml_prediction = predict_with_estimate(X_test, beta_ml)\n","\n","# plot\n","plt.figure()\n","plt.plot(X, ynew, '+', markersize=10)\n","plt.plot(X_test, ml_prediction)\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pAobMbOava5j"},"source":["#### Question:\n","1. This maximum likelihood estimate doesn't look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\n","2. How can we fix this problem?\n","\n","Let us now define a linear regression model that is slightly more flexible:\n","$$\n","y = \\beta_0 + \\boldsymbol x^T \\boldsymbol\\beta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n","$$\n","\n","Here, we added an offset (also called intercept) parameter $\\beta_0$ to our original model.\n","\n","#### Question:\n","1. What is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n","\n","If we now define the inputs to be the augmented vector $\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}$, we can write the new linear regression model as \n","$$\n","y = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\beta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\beta_{\\text{aug}} = \\begin{bmatrix}\n","\\beta_0\\\\\n","\\boldsymbol\\beta_1\n","\\end{bmatrix}\\,.\n","$$"]},{"cell_type":"code","metadata":{"id":"4ztZ-WZovcKh"},"source":["N, D = X.shape\n","X_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\n","beta_aug = np.zeros((D+1, 1)) # new beta vector of size (D+1) x 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Be_eR51IxS3X"},"source":["Let us now compute the maximum likelihood estimator for this setting.\n","\n","_Hint:_ If possible, re-use code that you have already written."]},{"cell_type":"code","metadata":{"id":"UFvLRt5KxQGz"},"source":["## EDIT THIS FUNCTION\n","def max_lik_estimate_aug(X_aug, y):\n","    \n","    beta_aug_ml = np.zeros((D+1,1)) ## <-- EDIT THIS LINE\n","    \n","    return beta_aug_ml"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVSuOa27xQDz"},"source":["beta_aug_ml = max_lik_estimate_aug(X_aug, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pxggOnL7xcSP"},"source":["Now, we can make predictions again:"]},{"cell_type":"code","metadata":{"id":"Fvjd3MQRxQA8"},"source":["# define a test set (we also need to augment the test inputs with ones)\n","X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test]) # 100 x (D + 1) vector of test inputs\n","\n","# predict the function values at the test points using the maximum likelihood estimator\n","ml_prediction = predict_with_estimate(X_test_aug, beta_aug_ml)\n","\n","# plot\n","plt.figure()\n","plt.plot(X, y, '+', markersize=10)\n","plt.plot(X_test, ml_prediction)\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\");"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6pTpinmQxi9g"},"source":["It seems this has solved our problem! \n","#### Question:\n","1. Play around with the first parameter of $\\boldsymbol\\beta_{\\text{aug}}$ and see how the fit of the function changes.\n","2. Play around with the second parameter of $\\boldsymbol\\beta_{\\text{aug}}$ and see how the fit of the function changes."]},{"cell_type":"markdown","metadata":{"id":"7K-gWeLFP15K"},"source":["## 2. Ridge regression\n","\n","From our lectures, we know that ridge regression is an extension of linear regression with least squares loss function, including a (usually small) positive penalty term $\\lambda$:\n","$$\n","\\underset{\\boldsymbol\\beta}{\\text{min}} \\| \\mathcal Y - \\mathcal X \\boldsymbol\\beta \\|^2 + \\lambda \\| \\boldsymbol\\beta \\|^2 = \\underset{\\boldsymbol\\beta}{\\text{min}} \\ \\text{L}_{\\text{ridge}} (\\boldsymbol\\beta)\n","$$\n","where $\\text{L}_{\\text{ridge}}$ is the ridge loss function. The solution is\n","$$\n","\\boldsymbol\\beta^{*}_{\\text{ridge}} = (\\boldsymbol X^T\\boldsymbol X + \\lambda I)^{-1}\\boldsymbol X^T\\boldsymbol y \\, .\n","$$\n","\n","\n","This time, we will define a very small training set of only two observations to demonstrate the advantages of ridge regression over least squares linear regression.\n","\n"]},{"cell_type":"code","metadata":{"id":"QDZPkTKK_Ylu"},"source":["X_train = np.array([0.5, 1]).reshape(-1,1)\n","y_train = [0.5, 1]\n","X_test = np.array([0, 2]).reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RvrNp3oP9K5"},"source":["Let's define function similar to the one for least squares, but taking one additional argument, our penalty term $\\lambda$. \n","\n","_Hint_: we apply the same augmentation as above with least squares, so the offset is accurately captured."]},{"cell_type":"code","metadata":{"id":"YhKvkUNeP6vz"},"source":["## EDIT THIS FUNCTION\n","def ridge_estimate(X, y, penalty):\n","    \n","    # X: N x D matrix of training inputs\n","    # y: N x 1 vector of training targets/observations\n","    # returns: maximum likelihood parameters (D x 1)\n","    \n","    N, D = X.shape\n","    X_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\n","    N_aug, D_aug = X_aug.shape\n","    I = np.identity(D_aug)\n","    beta_ridge = np.zeros((D+1,1)) ## <-- EDIT THIS LINE\n","    return beta_ridge"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPBRj7opQXGk"},"source":["Now, we add a bit of Gaussian noise to our training set and apply ridge regression. We should do it a couple of times to be sure about the results (here 10 times)."]},{"cell_type":"code","metadata":{"id":"wu0nBqDGQThT"},"source":["penalty_term = 0.1\n","fig, ax = plt.subplots(figsize=(12, 8))\n","X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test])\n","\n","for _ in range(10):\n","  this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train\n","\n","  beta_ridge = ridge_estimate(this_X, y_train, penalty=penalty_term)\n","  ridge_prediction = predict_with_estimate(X_test_aug, beta_ridge)\n","\n","  ax.plot(X_test, ridge_prediction, color='gray')\n","  ax.scatter(this_X, y_train, s=3, c='gray', marker='o', zorder=10)\n","\n","beta_ridge = ridge_estimate(X_train, y_train, penalty=penalty_term)\n","ridge_prediction_X = predict_with_estimate(X_test_aug, beta_ridge)\n","\n","ax.plot(X_test, ridge_prediction_X, linewidth=2, color='blue')\n","ax.scatter(X_train, y_train, s=30, c='red', marker='+', zorder=10);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvahgeXMQddU"},"source":["Let's compare this to ordinary least squares:"]},{"cell_type":"code","metadata":{"id":"M8R95vf6QTeV"},"source":["fig, ax = plt.subplots(figsize=(12, 8))\n","X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test])\n","\n","for _ in range(10):\n","  this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train\n","  N, D = this_X.shape\n","  this_X_aug = np.hstack([np.ones((N,1)), this_X])\n","\n","  beta_aug_ml = max_lik_estimate_aug(this_X_aug, y_train)\n","  ml_prediction = predict_with_estimate(X_test_aug, beta_aug_ml)\n","\n","  ax.plot(X_test, ml_prediction, color='gray')\n","  ax.scatter(this_X, y_train, s=3, c='gray', marker='o', zorder=10)\n","\n","beta_aug_ml = max_lik_estimate_aug(X_train, y_train)\n","ml_prediction_X = predict_with_estimate(X_test_aug, beta_ridge)\n","\n","ax.plot(X_test, ml_prediction_X, linewidth=2, color='blue')\n","ax.scatter(X_train, y_train, s=30, c='red', marker='+', zorder=10);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9yOoy_6RQiQ-"},"source":["#### Questions\n","1. What differences between the two solutions above can you see?\n","2. Play around with different values of the penalty term $\\lambda$. How do the corresponding functions change? Which values provide the most reasonable results?"]}]}