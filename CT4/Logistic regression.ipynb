{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic regression.ipynb","provenance":[{"file_id":"https://github.com/probml/pyprobml/blob/master/notebooks/intro/logreg.ipynb","timestamp":1600853677509}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"girxRWz391Xo"},"source":["# Logistic regression\n","The purpose of this notebook is to understand and implement logistic regression. As always, you are not allowed to use any package that has a complete logistic regression framework implemented (e.g., scikit-learn).\n","\n","Logistic regression, despite its name, is a linear model for classification rather than regression. In its original form, it is used for binary classifications, i.e., assigning a data point in our test set a binary label (e.g., yes or no, 0 or 1, red or blue). The reason why the term logistic *regression* is used becomes obvious once we examine the logistic function (often also called sigmoind function):\n","$$\n","f(x) = \\frac{1}{1+e^{-x}}\n","$$\n","Next, you will implement the logistic function using numpy."]},{"cell_type":"code","metadata":{"id":"pk6OgGZ391A_"},"source":["# importing standard packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_classification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0w3FhDQy909V"},"source":["## EDIT THIS FUNCTION\n","def logistic(x):\n","    return 1  ## <-- EDIT THIS LINE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eF8XcJUjBovk"},"source":["Let's plot the function to see how it behaves."]},{"cell_type":"code","metadata":{"id":"h2qqiNot906N"},"source":["plt.figure(figsize=(12,8))\n","x = np.linspace(-6, 6, 1000)\n","y = logistic(x)\n","plt.xlabel(r'$x$', size=20)\n","plt.ylabel(r'$y$', size=20)\n","plt.grid(alpha=0.5)\n","plt.plot(x, y);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ioR-FjI0cF-B"},"source":["#### Questions:\n","1. Can you already guess why this _regression_ model is used in _binary classification_ tasks?\n","2. What do the bounds of the logistic function tell you?\n","\n","Let's generate now a data set with sklearn's [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function:"]},{"cell_type":"code","metadata":{"id":"oOtP1llv90m7"},"source":["X, y = make_classification(n_samples=500, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=14)\n","plt.figure(figsize=(12,8))\n","plt.xlabel(r'$X_1$', size=20)\n","plt.ylabel(r'$X_2$', size=20)\n","plt.scatter(X[:,0],X[:,1], c=y.reshape(-1));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3AGfIQXMc38A"},"source":["We divide the data set into train and test sets and run our model with your own choice of hyperparameters:"]},{"cell_type":"code","metadata":{"id":"yxvfN7yI90kG"},"source":["# stacking data X and labels y into one matrix\n","data = np.hstack((X, y[:, np.newaxis]))\n","\n","# shuffling the rows        \n","np.random.shuffle(data)\n","\n","# we split train to test as 70:30\n","split_rate = 0.7\n","train, test = np.split(data, [int(split_rate*(data.shape[0]))])\n","\n","X_train = train[:,:-1].T\n","y_train = train[:, -1]\n","\n","X_test = test[:,:-1].T\n","y_test = test[:, -1]\n","\n","y_train = y_train.astype(int)\n","y_test = y_test.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oY1-6VPECXbt"},"source":["In logistic regression, we learn parameters $\\boldsymbol \\beta$ and $\\beta_0$ similar to linear regression, but pass the output through a logistic function in the end. We call the output of this operation $\\hat{\\boldsymbol y}_{\\text{log}}$:\n","$$\n","\\hat{\\boldsymbol y}_{\\text{log}} = f(\\boldsymbol \\beta^T \\boldsymbol X + \\beta_0)\n","$$\n","where $\\boldsymbol X = [X^{(1)}, X^{(2)}, \\dots, X^{(n)}]$, and $X^{(i)} \\in \\mathbb R^d$.\n","\n","Note that $f$ is again the logistic function and, consequently, we have a _probability_ of the given data point belonging to one of the two classes, let's say red and blue. Then, you simply label any data point with probability greater than $0.5$ red, and any data point with probability less or equal to $0.5$ blue.\n","\n","Implement this function in the next cell."]},{"cell_type":"code","metadata":{"id":"sw9IfJ9p901N"},"source":["## EDIT THIS FUNCTION\n","def predict_log(X, beta, beta_0):\n","  y_log = 1  ## <-- EDIT THIS LINE\n","  return y_log"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I2_zR28_Ng1T"},"source":["A common technique in machine learning is to initialise the parameters $\\boldsymbol \\beta$ and $\\beta_0$ randomly or with zeros; we do the latter here."]},{"cell_type":"code","metadata":{"id":"Cb9smGSNNuiT"},"source":["def initialise(d):\n","  \"\"\"    \n","  Argument:\n","  d: size of the beta vector (or number of parameters)\n","  \n","  Returns:\n","  beta: initialised vector of shape (d, 1)\n","  beta_0: initialised scalar (corresponds to the offset)\n","  \"\"\"\n","  \n","  beta = np.zeros(shape=(d, 1), dtype=np.float32)\n","  beta_0 = 0\n","  \n","  assert(beta.shape==(d, 1))\n","  assert(isinstance(beta_0, float) or isinstance(beta_0, int))\n","  \n","  return beta, beta_0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLEUlU6mLQ5G"},"source":["From our lectures we know that the cost function to be minimised is given by\n","$$\n","\\mathcal L = - \\frac{1}{n} \\sum_{i=1}^n y^{(i)} \\log(\\hat{y}_{\\text{log}}^{(i)}) + (1-y^{(i)}) \\log (1-\\hat{y}_{\\text{log}}^{(i)}) \\, .\n","$$\n","We will use gradient descent to optimise this cost function. The derivatives with respect to $\\boldsymbol \\beta$ and $\\beta_0$ are:\n","$$\n","\\frac{\\partial \\mathcal L}{\\partial \\boldsymbol \\beta} = \\frac{1}{n} \\sum_{i=1}^n X^{(i)} (\\hat{y}_{\\text{log}}^{(i)} - y^{(i)})^T\n","$$\n"," \n","$$\n","\\frac{\\partial \\mathcal L}{\\partial \\beta_0} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_{\\text{log}}^{(i)} - y^{(i)})\n","$$\n","\n","Implement the cost function and its derivatives in the next cell as part of a larger operation which we shall call `propagate`, which is often also called a _forward pass_."]},{"cell_type":"code","metadata":{"id":"MmzAljLx90ya"},"source":["## EDIT THIS FUNCTION\n","def propagate(X, y, beta, beta_0):\n","  \"\"\"\n","  Arguments:\n","  X: data of size (d, n)\n","  y: true label vector of size (1, n)\n","  beta: parameters, a numpy array of size (d, 1)\n","  beta_0: offset, a scalar\n","\n","  Returns:\n","  cost: negative log-likelihood cost for logistic regression\n","  dbeta: gradient of the loss with respect to beta\n","  dbeta_0: gradient of the loss with respect to beta_0\n","  \"\"\"\n","  n = X.shape[1]\n","  y_log = predict_log(X, beta, beta_0)\n","\n","  # cost function\n","  cost = (-1) * ...  ## <-- EDIT THIS LINE (hint: don't forget axis=1 when using np.mean)\n","\n","  # derivatives\n","  dbeta = (1/n) * ...  ## <-- EDIT THIS LINE\n","  dbeta_0 =  1  ## <-- SOLUTION\n","\n","  assert(dbeta.shape==beta.shape)\n","  assert(dbeta_0.dtype==float)\n","  cost = np.squeeze(cost)\n","  assert(cost.shape==())\n","  \n","  # store gradients in a dictionary\n","  grads = {\"dbeta\": dbeta, \"dbeta_0\": dbeta_0}\n","  \n","  return grads, cost"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5PbMBTDVkRi"},"source":["With these gradients and the cost, we can conduct the actual optimisation and update the $\\boldsymbol \\beta$ and $\\beta_0$ with a learning rate $\\alpha$, which we shall set to $0.005$. You are required to implement the updating procedure for $\\boldsymbol \\beta$\n","$$\n","\\boldsymbol \\beta = \\boldsymbol \\beta - \\alpha \\ \\frac{\\partial \\mathcal L}{\\partial \\boldsymbol \\beta}\n","$$\n","and for offset $\\beta_0$\n","$$\n","\\beta_0 = \\beta_0 - \\alpha \\ \\frac{\\partial \\mathcal L}{\\partial \\beta_0} \\, .\n","$$"]},{"cell_type":"code","metadata":{"id":"OtyWtc8r90vH"},"source":["## EDIT THIS FUNCTION\n","def optimise(X, y, beta, beta_0, num_iterations=1000, learning_rate=0.005, print_cost=False):\n","  \"\"\"\n","  Arguments:\n","  X: data of size (d, n)\n","  y: true label vector of size (1, n)\n","  beta: parameters, a numpy array of size (d, 1)\n","  beta_0: offset, a scalar\n","  num_iterations: number of iterations gradient descent shall update the parameters\n","  learning_rate: step size in updating procedure\n","  print_cost: whether to print the cost every 100 iterations or not\n","\n","  Returns:\n","  params: dictionary containing the parameters beta and offset beta_0\n","  grads: dictionary containing the gradients\n","  costs: list of all the costs computed during the optimisation (can be used to plot the learning curve).\n","  \"\"\"\n","  costs = []\n","    \n","  for i in range(num_iterations):\n","\n","      # calculate cost and gradients (hint: use your existing functions)\n","      grads, cost = ...  ## <-- EDIT THIS LINE\n","      \n","      # retrieve derivatives from grads\n","      dbeta = grads[\"dbeta\"]\n","      dbeta_0 = grads[\"dbeta_0\"]\n","      \n","      # updating procedure\n","      beta = 1  ## <-- EDIT THIS LINE\n","      beta_0 = 1  ## <-- EDIT THIS LINE\n","      \n","      # record the costs\n","      if i % 100 == 0:\n","          costs.append(cost)\n","      \n","      # print the cost every 100 iterations\n","      if print_cost and i % 100 == 0:\n","          print (\"cost after iteration %i: %f\" %(i, cost))\n","  \n","  # save parameters and gradients in dictionary\n","  params = {\"beta\": beta, \"beta_0\": beta_0}\n","  grads = {\"dbeta\": dbeta, \"dbeta_0\": dbeta_0}\n","  \n","  return params, grads, costs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bXh8JQXzYVZM"},"source":["Having calculated the parameters for our training sets, we can finally predict the labels for our test set."]},{"cell_type":"code","metadata":{"id":"EtVG0I2w90sk"},"source":["## EDIT THIS FUNCTION\n","def predict(X_test, beta, beta_0):\n","  \"\"\"\n","  Arguments:\n","  X_test: test data of size (d, n)\n","  beta: parameters, a numpy array of size (d, 1)\n","  beta_0: offset, a scalar\n","\n","  Returns:\n","  y_pred: vector containing all binary predictions (0/1) for the examples in X_test\n","  \"\"\"\n","  n = X_test.shape[1]\n","  y_pred = np.zeros((1,n))\n","  beta = beta.reshape(X_test.shape[0], 1)\n","  \n","  # compute vector y_log predicting the probabilities\n","  y_log = predict_log(X_test, beta, beta_0)\n","  \n","  for i in range(y_log.shape[1]):\n","      \n","      # convert probabilities y_log to actual predictions y_pred\n","      if y_log[0, i] > 0.5:\n","          y_pred[0, i] = 5  ## <-- EDIT THIS LINE\n","      else:\n","          y_pred[0, i] = -5  ## <-- EDIT THIS LINE\n","  \n","  assert(y_pred.shape==(1, n))\n","  \n","  return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9p1-IQcZ_aJ"},"source":["A pythonic way to define an algorithm is placing all functions in one model that has all hyperparameters as arguments. This allows you to quickly evaluate different hyperparameters and optimise over these. So, let's do this:"]},{"cell_type":"code","metadata":{"id":"GA_yAaPu90rI"},"source":["def model(X_train, y_train, X_test, y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n","  # initialize parameters with zeros\n","  beta, beta_0 = initialise(X_train.shape[0])\n","\n","  # gradient descent\n","  parameters, grads, costs = optimise(X_train, y_train, beta, beta_0, num_iterations, learning_rate, print_cost=print_cost)\n","\n","  # retrieve parameters beta and beta_0 from dictionary \"parameters\"\n","  beta = parameters[\"beta\"]\n","  beta_0 = parameters[\"beta_0\"]\n","\n","  # predict test and train set examples\n","  y_pred_test = predict(X_test, beta, beta_0)\n","  y_pred_train = predict(X_train, beta, beta_0)\n","\n","  # print train/test Errors\n","  print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_train - y_train)) * 100))\n","  print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_pred_test - y_test)) * 100))\n","\n","  # saving all information\n","  d = {\"costs\": costs, \"y_pred_test\": y_pred_test, \"y_pred_train\": y_pred_train, \"beta\": beta, \"beta_0\": beta_0, \"learning_rate\": learning_rate, \"num_iterations\": num_iterations}\n","  \n","  return d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u60bSkU790hY"},"source":["# run the model\n","d = model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.1, print_cost=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFQ3dtktpuR6"},"source":["Let's see how our cost has changed over the training iterations:"]},{"cell_type":"code","metadata":{"id":"6wa04Wc690c6"},"source":["costs = np.squeeze(d['costs'])\n","plt.figure(figsize=(12,8))\n","plt.ylabel('Cost', size=20)\n","plt.xlabel('Iterations (in hundreds)', size=20)\n","plt.title(\"Learning rate = \" + str(d[\"learning_rate\"]), size=20)\n","plt.plot(costs);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9etWossCprpL"},"source":["#### Questions:\n","1. What insights do you gain from this learning curve?\n","2. Try different learning rates, run the model again, and plot the learning curve. What can you observe?\n","3. Use different random states when you generate the data and run the model again. What can you observe?\n","4. Increase the number of features in your generated data and evaluate the accuracies again. How do they change?\n","5. Generate data with sklearn's [`make_moons`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function (noise $> 0.1$) and evaluate how well logistic regression performs on this data set."]},{"cell_type":"code","metadata":{"id":"UY_TdZ6E9z0X"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6Mbd5RkrPNL"},"source":[""],"execution_count":null,"outputs":[]}]}