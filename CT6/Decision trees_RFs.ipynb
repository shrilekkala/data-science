{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Decision trees.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPXAVjdWZQgm0npHLPX91oY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rcr6esW4Fv6_"},"source":["# 1. Decision trees\n","Decision trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of the label of unseen data by learning simple decision rules inferred from the data features.\n","\n","You can understand it as using a set of _if-then-else_ decision rules, e.g., _if_ it snowed in London, _then_ many Londoners would ski on Primrose Hill. _Else_, they would walk in Hyde Park.\n","Generally speaking, the deeper the tree, i.e., the more _if-then-else_ decisions are subsequently made in our model, the more complex the decision rules and the fitter the model. However, note that decision trees are prone to overfitting. \n","\n","In this notebook, we will use decistion trees as a classification algorithm with the Gini impurity, and work with the famous [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set). It contains four biological characteristics _(features)_ of 150 samples that belong to three species _(classes)_ of the family of Iris flowers (Iris setosa, Iris virginica and Iris versicolor). The data set provides 50 samples for each species."]},{"cell_type":"code","metadata":{"id":"PxlpS7eZFpdk"},"source":["# import packages\n","from sklearn.datasets import load_iris\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CIZVtIbyhNEe"},"source":["# load data\n","data = load_iris()\n","# print data to see how it is structured\n","#print(data)\n","X, y, column_names = data['data'], data['target'], data['feature_names']\n","# combining all information in one data frame\n","X_y = pd.DataFrame(X, columns=column_names)\n","X_y['label'] = y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWCJ9D5PhRQr"},"source":["# check\n","X_y.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uMp0Q3eEhyFi"},"source":["It is always a good idea to see whether the features are correlated. The python package `seaborn` has a nice one-line command to explore this visually. It directly prints the feature names (sepal length, sepal width, petal length, petal width) and labels as axis titles."]},{"cell_type":"code","metadata":{"id":"wQsi2D6ThtKU"},"source":["import seaborn as sns\n","\n","sns.pairplot(X_y);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UjOk2mjjooA"},"source":["As with any other supervised machine learning method, we create a train and test set to learn and evaluate our model, respectively."]},{"cell_type":"code","metadata":{"id":"QTMU873Dh_G-"},"source":["# stacking data X and labels y into one matrix\n","X_y_shuff = X_y.iloc[np.random.permutation(len(X_y))]\n","\n","# we split train to test as 70:30\n","split_rate = 0.7\n","train, test = np.split(X_y_shuff, [int(split_rate*(X_y_shuff.shape[0]))])\n","\n","X_train = train[train.columns[:-1]]\n","y_train = train[train.columns[-1]]\n","\n","X_test = test[test.columns[:-1]]\n","y_test = test[test.columns[-1]]\n","\n","y_train = y_train.astype(int)\n","y_test = y_test.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RD7QG1voKIkF"},"source":["We will build up our decision tree algorithm in the pythonic way how we have done it previously as well by calling functions we define first in functions we define later. For quick evaluations of your implementations, however, we have also included one-line commands after each cell to see whether your implementation results in errors.\n","\n","In our lectures, we have learnt how the Gini impurity of the labels $\\boldsymbol y$ is calculated:\n","$$\n","\\text{GI}(\\boldsymbol y) = 1 - \\sum_{i=1}^N \\mathbb P (y_i)^2\n","$$\n","\n","It's your turn to implement it in the next cell. We want to allow the code to consider samples with different weights, hence introduce an additional argument called `sample_weights`. The Iris data set we work with in this notebook has uniform sample weights, but other data sets you work with in the future could be different."]},{"cell_type":"code","metadata":{"id":"ihQAbjFxEZX4"},"source":["# EDIT THIS FUNCTION\n","def gini_impurity(y, sample_weights=None):\n","  \"\"\" \n","  Calculate the gini impurity for labels.\n","  Arguments:\n","      y: vector of training labels, of shape (N,).\n","      sample_weights: weights for each samples, of shape (N,).\n","  Returns:\n","      (float): the gini impurity for y.\n","  \"\"\"\n","  if sample_weights is None:\n","      sample_weights = np.ones(y.shape[0]) / y.shape[0]\n","  \n","  gini = 1\n","  num = y.shape[0]  # number of labels\n","  label_counts = {}  # caculate different labels in yï¼Œand store in label_counts\n","  for i in range(num):\n","      if y[i] not in label_counts.keys():\n","          label_counts[y[i]] = 0\n","      label_counts[y[i]] += sample_weights[i]\n","  \n","  for key in label_counts:\n","      prob = float(label_counts[key]) / float(np.sum(sample_weights))\n","      gini -= 2 ## <-- EDIT THIS LINE\n","\n","  return gini"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OoieamNKKjJ"},"source":["# evaluate labels y\n","gini_impurity(y_train.to_numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ByxQZ5BHOJIN"},"source":["Next, we define a function to split the data set. This has not much use yet, but we will call it in later functions, e.g., in the next cell in `gini_purification`."]},{"cell_type":"code","metadata":{"id":"bd8WFZLKF6iT"},"source":["def split_dataset(X, y, column, value, sample_weights=None):\n","  \"\"\"\n","  Return the split of data whose column-th feature equals value.\n","  Arguments:\n","      X: training features, of shape (N, D).\n","      y: vector of training labels, of shape (N,).\n","      column: the column of the feature for splitting.\n","      value: the value of the column-th feature for splitting.\n","      sample_weights: weights for each samples, of shape (N,).\n","  Returns:\n","      (np.array): the subset of X whose column-th feature equals value.\n","      (np.array): the subset of y whose column-th feature equals value.\n","      (np.array): the subset of sample weights whose column-th feature equals value.\n","  \"\"\" \n","  ret = []\n","  featVec = X[:, column]\n","  X = X[:,[i for i in range(X.shape[1]) if i!=column]]\n","  \n","  for i in range(len(featVec)):\n","      if featVec[i]==value:\n","          ret.append(i)\n","  \n","  sub_X = X[ret,:]\n","  sub_y = y[ret]\n","  sub_sample_weights = sample_weights[ret]\n","\n","  return sub_X, sub_y, sub_sample_weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EMkU0OTPGJ5"},"source":["We need a function to calculate the Gini impurity for every feature. Let's do this in the following cell by calling our previously defined two functions `split_dataset` and `gini_impurity`. "]},{"cell_type":"code","metadata":{"id":"FE1DqOsmHsUr"},"source":["# EDIT THIS FUNCTION\n","def gini_purification(X, y, column, sample_weights=None):\n","  \"\"\"\n","  Calculate the resulted gini impurity given a vector of features.\n","  Arguments:\n","      X: training features, of shape (N, D).\n","      y: vector of training labels, of shape (N,).\n","      column: the column of the feature for calculating. 0 <= column < D\n","      sample_weights: weights for each samples, of shape (N,).\n","  Returns:\n","      (float): the resulted gini impurity after splitting by this feature.\n","  \"\"\"\n","  if sample_weights is None:\n","      sample_weights = np.ones(y.shape[0]) / y.shape[0]\n","  \n","  new_impurity = 0\n","  old_cost = gini_impurity(y, sample_weights)\n","  \n","  unique_vals = np.unique(X[:, column])\n","  new_cost = 0.0\n","  #split the values of i-th feature and calculate the cost \n","  for value in unique_vals:\n","      sub_X, sub_y, sub_sample_weights = 1 ## <-- EDIT THIS LINE\n","      prob = np.sum(sub_sample_weights) / float(np.sum(sample_weights))\n","      new_cost += prob * 2 ## <-- EDIT THIS LINE\n","  \n","  new_impurity = old_cost - new_cost # information gain\n","\n","  return new_impurity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fWSvdhqP0iY"},"source":["# evaluate for feature sepal width (cm)\n","gini_purification(X_train.to_numpy(), y_train.to_numpy(), 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rVkI3wBVQ12B"},"source":["It's now time to choose the best feature to split by calling the function `gini_purification` for each feature."]},{"cell_type":"code","metadata":{"id":"FrUdNu3FEZUv"},"source":["## EDIT THIS FUNCTION\n","def choose_best_feature(X, y, sample_weights=None):\n","  \"\"\"\n","  Choose the best feature to split according to criterion.\n","  Args:\n","      X: training features, of shape (N, D).\n","      y: vector of training labels, of shape (N,).\n","      sample_weights: weights for each samples, of shape (N,).\n","  Returns:\n","      (int): the column for the best feature\n","  \"\"\"\n","  if sample_weights is None:\n","      sample_weights = np.ones(y.shape[0]) / y.shape[0]\n","\n","  best_feature_idx = 0\n","  n_features = X.shape[1]    \n","  \n","  # use C4.5 algorirhm\n","  best_gain_cost = 0.0\n","  for i in range(n_features):\n","      info_gain_cost = 2  ## <-- EDIT THIS LINE         \n","      if info_gain_cost > best_gain_cost:\n","          best_gain_cost = info_gain_cost\n","          best_feature_idx = i                \n","\n","  return best_feature_idx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxY8uqmHRc-N"},"source":["# evaluate which feature is best\n","choose_best_feature(X_train.to_numpy(), y_train.to_numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"trLa8CV7SF2I"},"source":["Now, we need a function that returns the label which appears the most in our label variable `y`."]},{"cell_type":"code","metadata":{"id":"F32t-yHzG864"},"source":["def majority_vote(y, sample_weights=None):\n","  \"\"\"\n","  Return the label which appears the most in y.\n","  Args:\n","      y: vector of training labels, of shape (N,).\n","      sample_weights: weights for each samples, of shape (N,).\n","  Returns:\n","      (int): the majority label\n","  \"\"\"\n","  if sample_weights is None:\n","      sample_weights = np.ones(y.shape[0]) / y.shape[0]\n","  \n","  majority_label = y[0]\n","\n","  dict_num = {}\n","  for i in range(y.shape[0]):\n","      if y[i] not in dict_num.keys():\n","          dict_num[y[i]] = sample_weights[i]\n","      else:\n","          dict_num[y[i]] += sample_weights[i]\n","  \n","  majority_label = max(dict_num, key=dict_num.get)\n","  # end answer\n","  return majority_label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fZzFMoQtS4tR"},"source":["# evaluate it\n","majority_vote(y_train.to_numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nHihzl6CS9X0"},"source":["Finally, we can build the decision tree by using `choose_best_feature` to find the best feature to split the `X`, and `split_dataset` to get sub-trees."]},{"cell_type":"code","metadata":{"id":"fxsU-BWeEZRu"},"source":["# EDIT THIS FUNCTION\n","def build_tree(X, y, feature_names, depth, sample_weights=None, max_depth=10, min_samples_leaf=2):\n","  \"\"\"Build the decision tree according to the data.\n","  Args:\n","      X: (np.array) training features, of shape (N, D).\n","      y: (np.array) vector of training labels, of shape (N,).\n","      feature_names (list): record the name of features in X in the original dataset.\n","      depth (int): current depth for this node.\n","      sample_weights: weights for each samples, of shape (N,).\n","  Returns:\n","      (dict): a dict denoting the decision tree. \n","      Example:\n","          The first best feature name is 'title', and it has 5 different values: 0,1,2,3,4. For 'title' == 4, the next best feature name is 'pclass', we continue split the remain data. If it comes to the leaf, we use the majority_label by calling majority_vote.\n","          mytree = {\n","              'titile': {\n","                  0: subtree0,\n","                  1: subtree1,\n","                  2: subtree2,\n","                  3: subtree3,\n","                  4: {\n","                      'pclass': {\n","                          1: majority_vote([1, 1, 1, 1]) # which is 1, majority_label\n","                          2: majority_vote([1, 0, 1, 1]) # which is 1\n","                          3: majority_vote([0, 0, 0]) # which is 0\n","                      }\n","                  }\n","              }\n","          }\n","  \"\"\"\n","  mytree = dict()\n","\n","  # include a clause for the cases where (i) no feature, (ii) all lables are the same, (iii) depth exceed, or (iv) X is too small\n","  if len(feature_names)==0 or len(np.unique(y))==1 or depth>=max_depth or len(X)<=min_samples_leaf: \n","      return majority_vote(y, sample_weights)\n","  \n","  else:\n","    best_feature_idx = 1  ## <-- EDIT THIS LINE\n","    best_feature_name = feature_names[best_feature_idx]\n","    feature_names = feature_names[:]\n","    feature_names.remove(best_feature_name)\n","    \n","    mytree = {best_feature_name:{}}\n","    unique_vals = np.unique(X[:, best_feature_idx])\n","    for value in unique_vals:\n","        sub_X, sub_y, sub_sample_weights = split_dataset(X, y, best_feature_idx, value, sample_weights)  ## <-- EDIT THIS LINE\n","        mytree[best_feature_name][value] = build_tree(sub_X, sub_y, feature_names, depth+1, sub_sample_weights)  ## <-- EDIT THIS LINE (hint: use depth+1 as the depth)\n","\n","    return mytree"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9VZ5LpqAUqaa"},"source":["We define a wrapper function that we call `train` to call this `build_tree` function with the appropriate arguments."]},{"cell_type":"code","metadata":{"id":"VwqMWBa3EZOW"},"source":["def train(X, y, sample_weights=None):\n","  \"\"\"\n","  Build the decision tree according to the training data.\n","  Args:\n","      X: (pd.Dataframe) training features, of shape (N, D). Each X[i] is a training sample.\n","      y: (pd.Series) vector of training labels, of shape (N,). y[i] is the label for X[i], and each y[i] is\n","      an integer in the range 0 <= y[i] <= C. Here C = 1.\n","      sample_weights: weights for each samples, of shape (N,).\n","  \"\"\"\n","  if sample_weights is None:\n","      # if the sample weights is not provided, we assume the samples have uniform weights\n","      sample_weights = np.ones(X.shape[0]) / X.shape[0]\n","  else:\n","      sample_weights = np.array(sample_weights) / np.sum(sample_weights)\n","\n","  feature_names = X.columns.tolist()\n","  X = np.array(X)\n","  y = np.array(y)\n","  tree = build_tree(X, y, feature_names, depth=1, sample_weights=sample_weights)\n","  return tree"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1T3PFpjEZLd"},"source":["# fit the decision tree with training data\n","tree = train(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAE19BbTXYJb"},"source":["Now, we want to use this fitted decision tree to make predictions for our test set `X_test`. To do so, we first define a function `classify` that takes each single data point `x` as an argument. We will write a wrapper function `predict` that calls this `classify` function."]},{"cell_type":"code","metadata":{"id":"QIJYH7hNWOd-"},"source":["def classify(tree, x):\n","  \"\"\"\n","  Classify a single sample with the fitted decision tree.\n","  Args:\n","      x: ((pd.Dataframe) a single sample features, of shape (D,).\n","  Returns:\n","      (int): predicted testing sample label.\n","  \"\"\"\n","  feature_name = list(tree.keys())[0] # first element\n","  second_dict = tree[feature_name]            \n","  key = x.loc[feature_name]\n","  if key not in second_dict:\n","      key = np.random.choice(list(second_dict.keys()))\n","  value_of_key = second_dict[key]\n","  if isinstance(value_of_key, dict):\n","      label = classify(value_of_key, x)\n","  else:\n","      label=value_of_key\n","  return label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_WNC05qWOYG"},"source":["def predict(X):\n","  \"\"\"\n","  Predict classification results for X.\n","  Args:\n","      X: (pd.Dataframe) testing sample features, of shape (N, D).\n","  Returns:\n","      (np.array): predicted testing sample labels, of shape (N,).\n","  \"\"\"\n","  if len(X.shape)==1:\n","      return classify(tree, X)\n","  else:\n","      results=[]\n","      for i in range(X.shape[0]):\n","          results.append(classify(tree, X.iloc[i, :]))\n","      return np.array(results)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-yiJR4aQX2RI"},"source":["To evaluate how well the tree can generalise to unseen data in `X_test`, we define a short function that computes the mean accuracy."]},{"cell_type":"code","metadata":{"id":"WfaBgIhaEZH_"},"source":["## EDIT THIS FUNCTION\n","def score(X_test, y_test):\n","  y_pred = 2 ## <-- EDIT THIS LINE\n","  return np.float(sum(y_pred==y_test)) / float(len(y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FX4nyXObWjI0"},"source":["print('Training accuracy:', score(X_train, y_train))\n","print('Test accuracy:', score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jaWlcZAZYQrv"},"source":["#### Questions:\n","1. What do the results above tell you? Has the decision tree you implemented overfitted to the training data?\n","2. If so, what can you change to counteract this problem?\n","3. Can you think of other information criteria than the Gini impurity? If so, try implementing them and compare it to the decision tree with the Gini impurity.\n","\n"]},{"cell_type":"code","metadata":{"id":"4labANV6WjQF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"twRCXMEC9sv3"},"source":[""],"execution_count":null,"outputs":[]}]}