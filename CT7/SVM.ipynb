{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNAE0YovIsgwcgTpjvfKXtD"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Arz0uQUpJ8nx"},"source":["# Support Vector Machines (SVMs)\n","In this notebook, we will learn a linear and kernalised method of SVMs, which can be used for both regression and classification. To start with, we will focus on binary classification. We will use stochastic gradient descent (SGD) for the optimisation of the hinge loss.\n","\n","We will work with the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data), which you first need to download and then load in this notebook. If you faced difficulties downloading this data set from Kaggle, you should download the file directly from Blackboard. The data set contains various aspects of cell nuclei of breast screening images of patients with _(malignant)_ and without _(benign)_ breast cancer. Our goal is to build a classification model that can take these aspects of an unseen breast screening image, and classify it as either malignant or benign.\n","\n","If you run this notebook locally on your machine, you will simply need to place the `csv` file in the same directory as this notebook.\n","If you run this notebook on Google Colab, you will need to use\n","\n","  `from google.colab import files`\n","\n","  `upload = files.upload()`\n","\n","and then upload it from your local downloads directory."]},{"cell_type":"code","metadata":{"id":"oeRdgi8X2_kD"},"source":["# necessary imports\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-9FsQfQ1J4MI"},"source":["data = pd.read_csv('./data.csv')\n","\n","# print shape and last 10 rows\n","print(data.shape)\n","data.tail(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"74Tw6xk53p20"},"source":["We can see that our data set has 569 samples and 33 columns. The column `id` can be taken as an index for our pandas dataframe and `diagnosis` is the label (either **M: malignant** or **B: benign**).\n","\n","Let's prepare the data set first of all by (i) cleaning it, (ii) separating label from features, and (iii) splitting it into train and test sets."]},{"cell_type":"code","metadata":{"id":"2mbvvJ-Kz2Dk"},"source":["# drop last column (extra column added by pd)\n","data_1 = data.drop(data.columns[-1], axis=1)\n","# set column id as dataframe index\n","data_2 = data_1.set_index(data['id']).drop(data_1.columns[0], axis=1)\n","\n","# check\n","data_2.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zQ1tedL7dNI"},"source":["We do a bit more preparation by converting the categorical labels into 1 for **M** and -1 for **B**."]},{"cell_type":"code","metadata":{"id":"1zbSOnY06rwL"},"source":["# convert categorical labels to numbers\n","diag_map = {'M': 1.0, 'B': -1.0}\n","data_2['diagnosis'] = data_2['diagnosis'].map(diag_map)\n","\n","# put labels and features in different dataframes\n","y = data_2.loc[:, 'diagnosis']\n","X = data_2.iloc[:, 1:]\n","\n","# check\n","print(y.tail())\n","X.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PbllXkPK8fQw"},"source":["As with any data set that has features over different ranges, it's required to standardise the data before."]},{"cell_type":"code","metadata":{"id":"PSVn27p88EFl"},"source":["## EDIT THIS FUNCTION\n","def standardise(X):\n","  mu = np.mean(X, 0)\n","  sigma = np.std(X, 0)\n","  X_std = 2 ## <-- EDIT THIS LINE\n","  return X_std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMyxlg678EC7"},"source":["X_std = standardise(X)\n","\n","# check\n","X_std.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcw02XXM8EAe"},"source":["# insert 1 in every row for intercept b\n","X_std.insert(loc=len(X_std.columns), column='intercept', value=1)\n","\n","# split into train and test set\n","# stacking data X and labels y into one matrix\n","data_split = np.hstack((X_std, y[:, np.newaxis]))\n","\n","# shuffling the rows        \n","np.random.shuffle(data_split)\n","\n","# we split train to test as 70:30\n","split_rate = 0.7\n","train, test = np.split(data_split, [int(split_rate*(data_split.shape[0]))])\n","\n","X_train = train[:,:-1]\n","y_train = train[:, -1]\n","\n","X_test = test[:,:-1]\n","y_test = test[:, -1]\n","\n","y_train = y_train.astype(float)\n","y_test = y_test.astype(float)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jMo_v3TGCJBD"},"source":["## Linear SVM\n","We start with defining the hinge loss as\n","$$\n","\\mathcal L (\\boldsymbol w) = \\frac{1}{2} \\| \\boldsymbol w \\|^2 + \\frac{\\lambda}{n} \\sum_{i=1}^n \\max \\bigg( 0, 1-y_i (\\boldsymbol w \\cdot x_i + b) \\bigg) \\, .\n","$$\n","where $\\boldsymbol w$ is the vector of weights, $\\lambda$ the regularisation parameter, and $b$ the intercept which is included in our `X` as an additional column of $1$'s."]},{"cell_type":"code","metadata":{"id":"O8NCZ2Wj8D8m"},"source":["# EDIT THIS FUNCTION\n","def compute_cost(W, X, y, regul_strength=1e5):\n","  n = X.shape[0]\n","  distances = 1 - ...  ## <-- EDIT THIS LINE\n","  distances[distances < 0] = 0  # equivalent to max(0, distance)\n","  hinge = regul_strength * ...  ## <-- EDIT THIS LINE\n","\n","  # calculate cost\n","  cost = 1 / 2 * np.dot(W, W) + hinge\n","  return cost"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPlQQlNJHgGI"},"source":["Next, we need the gradients of this cost function."]},{"cell_type":"code","metadata":{"id":"yvsu7ukAE79Y"},"source":["# calculate gradient of cost\n","def calculate_cost_gradient(W, X_batch, y_batch, regul_strength=1e5):\n","  # if only one example is passed\n","  if type(y_batch) == np.float64:\n","      y_batch = np.asarray([y_batch])\n","      X_batch = np.asarray([X_batch])  # gives multidimensional array\n","\n","  distance = 1 - (y_batch * np.dot(X_batch, W))\n","  dw = np.zeros(len(W))\n","\n","  for ind, d in enumerate(distance):\n","      if max(0, d)==0:\n","          di = W\n","      else:\n","          di = W - (regul_strength * y_batch[ind] * X_batch[ind])\n","      dw += di\n","\n","  dw = dw/len(y_batch)  # average\n","  return dw"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5X3_B9V-I0mX"},"source":["Both of the two previous functions are then used in SGD to update the weights iteratively with a given learning rate $\\alpha$. We also implement a stop criterion that ends the learning as soon as the cost function has not changed more than a manually determined percentage.\n","\n","We know that the learning happens through updating the weights according to\n","$$\n","\\boldsymbol w = \\boldsymbol w - \\alpha \\frac{\\partial \\mathcal L}{\\partial \\boldsymbol w}\n","$$\n","\n","where $\\frac{\\partial \\mathcal L}{\\partial \\boldsymbol w}$ is the gradient of the hinge loss we have computed in the previous cell."]},{"cell_type":"code","metadata":{"id":"a-S8N9C78D5R"},"source":["# EDIT THIS FUNCTION\n","def sgd(X, y, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=1e5, print_outcome=False):\n","  # initialise zero weights\n","  weights = np.zeros(X.shape[1])\n","  nth = 0\n","  # initialise starting cost as infinity\n","  prev_cost = np.inf\n","  \n","  # stochastic gradient descent\n","  for iteration in range(1, max_iterations):\n","      # shuffle to prevent repeating update cycles\n","      np.random.shuffle([X, y])\n","      for ind, x in enumerate(X):\n","          ascent = ... ## <-- EDIT THIS LINE\n","          weights = weights - (learning_rate * ascent)\n","\n","      # convergence check on 2^n'th iteration\n","      if iteration==2**nth or iteration==max_iterations-1:\n","          # compute cost\n","          cost = ...  ## <-- EDIT THIS LINE\n","          if print_outcome:\n","            print(\"Iteration is: {}, Cost is: {}\".format(iteration, cost))\n","          # stop criterion\n","          if abs(prev_cost - cost) < stop_criterion * prev_cost:\n","              return weights\n","          \n","          prev_cost = cost\n","          nth += 1\n","  \n","  return weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTU9fyVYK8ay"},"source":["Now, we can take these functions and train a linear SVM with our training data."]},{"cell_type":"code","metadata":{"id":"2bdsUvtu8D2f"},"source":["# train the model\n","W = sgd(X_train, y_train, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-3, regul_strength=1e3, print_outcome=True)\n","print(\"Training finished.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7GQqts4_O1WE"},"source":["To evaluate the mean accuracy in both train and test set, we write a small function called `score`."]},{"cell_type":"code","metadata":{"id":"5JahERSXOtJj"},"source":["## EDIT THIS FUNCTION\n","def score(W, X, y):\n","  y_preds = np.array([])\n","  for i in range(X.shape[0]):\n","    y_pred = np.sign(np.dot(X[i], W))\n","    y_preds = np.append(y_preds, y_pred)\n","  \n","  return np.float(sum(4)) / float(len(y)) ## <-- EDIT THIS LINE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZAt83Mq8Dv6"},"source":["print(\"Accuracy on train set: {}\".format(score(W, X_train, y_train)))\n","print(\"Accuracy on test set: {}\".format(score(W, X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CePz4SbER-Qb"},"source":["#### Questions:\n","1. What are other evaluation metrices besides the accuracy? Implement them and assess the performance of our classification algorithm with them.\n","2. What makes other evaluation metrices more appropriate given our unbalanced data set _(we have more benign than malignant examples)_?\n","3. Try different learning rates, regularisation strengths and number of iterations independently. What can you observe? Can you achieve higher accuracies?\n","4. What is your understanding why have we used the hinge loss with this data set of 31 features? \n","5. Can you think of other loss functions instead of the hinge loss? What is your intuition how they will perform compared to the hinge loss? You could try implementing one and compare the results. "]},{"cell_type":"markdown","metadata":{"id":"5mz-IlM3mHGv"},"source":["## *T*-fold cross validation\n","\n","Now we repeat the same procedure as above but do not only have one train-test split, but multiple in a *T*-fold cross validation method."]},{"cell_type":"code","metadata":{"id":"3wk-Bov5K2we"},"source":["def cross_val_split(data, num_folds):\n","  fold_size = int(len(data) / num_folds)\n","  data_perm = np.random.permutation(data)\n","  folds = []\n","  for k in range(num_folds):\n","    folds.append(data_perm[k*fold_size:(k+1)*fold_size, :])\n","\n","  return folds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rEkIneWmK-b"},"source":["# evaluate\n","folds = cross_val_split(train, 5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yjf6Xa-QmK7s"},"source":["## EDIT THIS FUNCTION\n","def cross_val_evaluate(data, num_folds):\n","  \n","  folds = cross_val_split(data, num_folds)\n","\n","  train_scores = []\n","  val_scores = []\n","\n","  for i in range(len(folds)):\n","    print('Fold', i+1)\n","    # define the training set\n","    train_set = np.delete(np.asarray(folds).reshape(len(folds), folds[0].shape[0], folds[0].shape[1]), i, axis=0)\n","    train_folds = train_set.reshape(len(train_set)*train_set[0].shape[0], train_set[0].shape[1])\n","    X_train = train_folds[:,  1]  ## <-- EDIT THIS LINE\n","    y_train = train_folds[:, -1]\n","    \n","    # define the validation set\n","    val_fold = folds[0]  ## <-- EDIT THIS LINE\n","    X_val = val_fold[:,  1]  ## <-- EDIT THIS LINE\n","    y_val = val_fold[:, -1]\n","\n","    # train the model\n","    W = sgd(X_train, y_train, max_iterations=1025, stop_criterion=0.01, learning_rate=1e-3, regul_strength=1e3)\n","    print(\"Training finished.\")\n","\n","    # evaluate\n","    train_score = score(W, X_train, y_train)\n","    val_score = score(W, X_val, y_val)\n","    print(\"Accuracy on train set #{}: {}\".format(i+1, train_score))\n","    print(\"Accuracy on validation set #{}: {}\".format(i+1, val_score))\n","\n","    train_scores.append(train_score)\n","    val_scores.append(val_score)\n","\n","  return train_scores, val_scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"poufUfxFmK47"},"source":["train_scores, val_scores = cross_val_evaluate(train, 5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0V8qyYIgmPsk"},"source":["Finally, let's compute the mean accuracy."]},{"cell_type":"code","metadata":{"id":"ovZTLlNRmJmC"},"source":["print(np.mean(val_scores))"],"execution_count":null,"outputs":[]}]}