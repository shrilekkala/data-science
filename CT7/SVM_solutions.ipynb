{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM_solutions.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfIPHwJuJJg1Z21qgYKMbW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Arz0uQUpJ8nx"},"source":["# Support Vector Machines (SVMs)\n","In this notebook, we will learn a linear and kernalised method of SVMs, which can be used for both regression and classification. To start with, we will focus on binary classification. We will use stochastic gradient descent (SGD) for the optimisation of the hinge loss.\n","\n","We will work with the [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data), which you first need to download and then load in this notebook. If you faced difficulties downloading this data set from Kaggle, you should download the file directly from Blackboard. The data set contains various aspects of cell nuclei of breast screening images of patients with _(malignant)_ and without _(benign)_ breast cancer. Our goal is to build a classification model that can take these aspects of an unseen breast screening image, and classify it as either malignant or benign.\n","\n","If you run this notebook locally on your machine, you will simply need to place the `csv` file in the same directory as this notebook.\n","If you run this notebook on Google Colab, you will need to use\n","\n","  `from google.colab import files`\n","\n","  `upload = files.upload()`\n","\n","and then upload it from your local downloads directory."]},{"cell_type":"code","metadata":{"id":"oeRdgi8X2_kD","executionInfo":{"status":"ok","timestamp":1612549868042,"user_tz":0,"elapsed":494,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["# necessary imports\n","import numpy as np\n","import pandas as pd\n","import copy"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":"OK"}},"base_uri":"https://localhost:8080/","height":79},"id":"iFFqFa9PKZYG","executionInfo":{"status":"ok","timestamp":1612548580464,"user_tz":0,"elapsed":7508,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"534e3b7b-f52c-414e-ad9e-1285ef97f381"},"source":[" from google.colab import files\n","\n","upload = files.upload()"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-22b6e841-63c5-49e8-9ba3-84bea78b9bb2\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-22b6e841-63c5-49e8-9ba3-84bea78b9bb2\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving data.csv to data (1).csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":418},"id":"-9FsQfQ1J4MI","executionInfo":{"status":"ok","timestamp":1612548585499,"user_tz":0,"elapsed":511,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"4d46b9bb-f382-4f89-dcc6-4e4660c290c1"},"source":["data = pd.read_csv('./data.csv')\n","\n","# print shape and last 10 rows\n","print(data.shape)\n","data.tail(10)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(569, 33)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>fractal_dimension_mean</th>\n","      <th>radius_se</th>\n","      <th>texture_se</th>\n","      <th>perimeter_se</th>\n","      <th>area_se</th>\n","      <th>smoothness_se</th>\n","      <th>compactness_se</th>\n","      <th>concavity_se</th>\n","      <th>concave points_se</th>\n","      <th>symmetry_se</th>\n","      <th>fractal_dimension_se</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","      <th>Unnamed: 32</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>559</th>\n","      <td>925291</td>\n","      <td>B</td>\n","      <td>11.51</td>\n","      <td>23.93</td>\n","      <td>74.52</td>\n","      <td>403.5</td>\n","      <td>0.09261</td>\n","      <td>0.10210</td>\n","      <td>0.11120</td>\n","      <td>0.04105</td>\n","      <td>0.1388</td>\n","      <td>0.06570</td>\n","      <td>0.2388</td>\n","      <td>2.904</td>\n","      <td>1.936</td>\n","      <td>16.97</td>\n","      <td>0.008200</td>\n","      <td>0.029820</td>\n","      <td>0.05738</td>\n","      <td>0.01267</td>\n","      <td>0.01488</td>\n","      <td>0.004738</td>\n","      <td>12.480</td>\n","      <td>37.16</td>\n","      <td>82.28</td>\n","      <td>474.2</td>\n","      <td>0.12980</td>\n","      <td>0.25170</td>\n","      <td>0.3630</td>\n","      <td>0.09653</td>\n","      <td>0.2112</td>\n","      <td>0.08732</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>560</th>\n","      <td>925292</td>\n","      <td>B</td>\n","      <td>14.05</td>\n","      <td>27.15</td>\n","      <td>91.38</td>\n","      <td>600.4</td>\n","      <td>0.09929</td>\n","      <td>0.11260</td>\n","      <td>0.04462</td>\n","      <td>0.04304</td>\n","      <td>0.1537</td>\n","      <td>0.06171</td>\n","      <td>0.3645</td>\n","      <td>1.492</td>\n","      <td>2.888</td>\n","      <td>29.84</td>\n","      <td>0.007256</td>\n","      <td>0.026780</td>\n","      <td>0.02071</td>\n","      <td>0.01626</td>\n","      <td>0.02080</td>\n","      <td>0.005304</td>\n","      <td>15.300</td>\n","      <td>33.17</td>\n","      <td>100.20</td>\n","      <td>706.7</td>\n","      <td>0.12410</td>\n","      <td>0.22640</td>\n","      <td>0.1326</td>\n","      <td>0.10480</td>\n","      <td>0.2250</td>\n","      <td>0.08321</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>561</th>\n","      <td>925311</td>\n","      <td>B</td>\n","      <td>11.20</td>\n","      <td>29.37</td>\n","      <td>70.67</td>\n","      <td>386.0</td>\n","      <td>0.07449</td>\n","      <td>0.03558</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.1060</td>\n","      <td>0.05502</td>\n","      <td>0.3141</td>\n","      <td>3.896</td>\n","      <td>2.041</td>\n","      <td>22.81</td>\n","      <td>0.007594</td>\n","      <td>0.008878</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.01989</td>\n","      <td>0.001773</td>\n","      <td>11.920</td>\n","      <td>38.30</td>\n","      <td>75.19</td>\n","      <td>439.6</td>\n","      <td>0.09267</td>\n","      <td>0.05494</td>\n","      <td>0.0000</td>\n","      <td>0.00000</td>\n","      <td>0.1566</td>\n","      <td>0.05905</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>562</th>\n","      <td>925622</td>\n","      <td>M</td>\n","      <td>15.22</td>\n","      <td>30.62</td>\n","      <td>103.40</td>\n","      <td>716.9</td>\n","      <td>0.10480</td>\n","      <td>0.20870</td>\n","      <td>0.25500</td>\n","      <td>0.09429</td>\n","      <td>0.2128</td>\n","      <td>0.07152</td>\n","      <td>0.2602</td>\n","      <td>1.205</td>\n","      <td>2.362</td>\n","      <td>22.65</td>\n","      <td>0.004625</td>\n","      <td>0.048440</td>\n","      <td>0.07359</td>\n","      <td>0.01608</td>\n","      <td>0.02137</td>\n","      <td>0.006142</td>\n","      <td>17.520</td>\n","      <td>42.79</td>\n","      <td>128.70</td>\n","      <td>915.0</td>\n","      <td>0.14170</td>\n","      <td>0.79170</td>\n","      <td>1.1700</td>\n","      <td>0.23560</td>\n","      <td>0.4089</td>\n","      <td>0.14090</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>563</th>\n","      <td>926125</td>\n","      <td>M</td>\n","      <td>20.92</td>\n","      <td>25.09</td>\n","      <td>143.00</td>\n","      <td>1347.0</td>\n","      <td>0.10990</td>\n","      <td>0.22360</td>\n","      <td>0.31740</td>\n","      <td>0.14740</td>\n","      <td>0.2149</td>\n","      <td>0.06879</td>\n","      <td>0.9622</td>\n","      <td>1.026</td>\n","      <td>8.758</td>\n","      <td>118.80</td>\n","      <td>0.006399</td>\n","      <td>0.043100</td>\n","      <td>0.07845</td>\n","      <td>0.02624</td>\n","      <td>0.02057</td>\n","      <td>0.006213</td>\n","      <td>24.290</td>\n","      <td>29.41</td>\n","      <td>179.10</td>\n","      <td>1819.0</td>\n","      <td>0.14070</td>\n","      <td>0.41860</td>\n","      <td>0.6599</td>\n","      <td>0.25420</td>\n","      <td>0.2929</td>\n","      <td>0.09873</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>564</th>\n","      <td>926424</td>\n","      <td>M</td>\n","      <td>21.56</td>\n","      <td>22.39</td>\n","      <td>142.00</td>\n","      <td>1479.0</td>\n","      <td>0.11100</td>\n","      <td>0.11590</td>\n","      <td>0.24390</td>\n","      <td>0.13890</td>\n","      <td>0.1726</td>\n","      <td>0.05623</td>\n","      <td>1.1760</td>\n","      <td>1.256</td>\n","      <td>7.673</td>\n","      <td>158.70</td>\n","      <td>0.010300</td>\n","      <td>0.028910</td>\n","      <td>0.05198</td>\n","      <td>0.02454</td>\n","      <td>0.01114</td>\n","      <td>0.004239</td>\n","      <td>25.450</td>\n","      <td>26.40</td>\n","      <td>166.10</td>\n","      <td>2027.0</td>\n","      <td>0.14100</td>\n","      <td>0.21130</td>\n","      <td>0.4107</td>\n","      <td>0.22160</td>\n","      <td>0.2060</td>\n","      <td>0.07115</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>565</th>\n","      <td>926682</td>\n","      <td>M</td>\n","      <td>20.13</td>\n","      <td>28.25</td>\n","      <td>131.20</td>\n","      <td>1261.0</td>\n","      <td>0.09780</td>\n","      <td>0.10340</td>\n","      <td>0.14400</td>\n","      <td>0.09791</td>\n","      <td>0.1752</td>\n","      <td>0.05533</td>\n","      <td>0.7655</td>\n","      <td>2.463</td>\n","      <td>5.203</td>\n","      <td>99.04</td>\n","      <td>0.005769</td>\n","      <td>0.024230</td>\n","      <td>0.03950</td>\n","      <td>0.01678</td>\n","      <td>0.01898</td>\n","      <td>0.002498</td>\n","      <td>23.690</td>\n","      <td>38.25</td>\n","      <td>155.00</td>\n","      <td>1731.0</td>\n","      <td>0.11660</td>\n","      <td>0.19220</td>\n","      <td>0.3215</td>\n","      <td>0.16280</td>\n","      <td>0.2572</td>\n","      <td>0.06637</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>566</th>\n","      <td>926954</td>\n","      <td>M</td>\n","      <td>16.60</td>\n","      <td>28.08</td>\n","      <td>108.30</td>\n","      <td>858.1</td>\n","      <td>0.08455</td>\n","      <td>0.10230</td>\n","      <td>0.09251</td>\n","      <td>0.05302</td>\n","      <td>0.1590</td>\n","      <td>0.05648</td>\n","      <td>0.4564</td>\n","      <td>1.075</td>\n","      <td>3.425</td>\n","      <td>48.55</td>\n","      <td>0.005903</td>\n","      <td>0.037310</td>\n","      <td>0.04730</td>\n","      <td>0.01557</td>\n","      <td>0.01318</td>\n","      <td>0.003892</td>\n","      <td>18.980</td>\n","      <td>34.12</td>\n","      <td>126.70</td>\n","      <td>1124.0</td>\n","      <td>0.11390</td>\n","      <td>0.30940</td>\n","      <td>0.3403</td>\n","      <td>0.14180</td>\n","      <td>0.2218</td>\n","      <td>0.07820</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>567</th>\n","      <td>927241</td>\n","      <td>M</td>\n","      <td>20.60</td>\n","      <td>29.33</td>\n","      <td>140.10</td>\n","      <td>1265.0</td>\n","      <td>0.11780</td>\n","      <td>0.27700</td>\n","      <td>0.35140</td>\n","      <td>0.15200</td>\n","      <td>0.2397</td>\n","      <td>0.07016</td>\n","      <td>0.7260</td>\n","      <td>1.595</td>\n","      <td>5.772</td>\n","      <td>86.22</td>\n","      <td>0.006522</td>\n","      <td>0.061580</td>\n","      <td>0.07117</td>\n","      <td>0.01664</td>\n","      <td>0.02324</td>\n","      <td>0.006185</td>\n","      <td>25.740</td>\n","      <td>39.42</td>\n","      <td>184.60</td>\n","      <td>1821.0</td>\n","      <td>0.16500</td>\n","      <td>0.86810</td>\n","      <td>0.9387</td>\n","      <td>0.26500</td>\n","      <td>0.4087</td>\n","      <td>0.12400</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>568</th>\n","      <td>92751</td>\n","      <td>B</td>\n","      <td>7.76</td>\n","      <td>24.54</td>\n","      <td>47.92</td>\n","      <td>181.0</td>\n","      <td>0.05263</td>\n","      <td>0.04362</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.1587</td>\n","      <td>0.05884</td>\n","      <td>0.3857</td>\n","      <td>1.428</td>\n","      <td>2.548</td>\n","      <td>19.15</td>\n","      <td>0.007189</td>\n","      <td>0.004660</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.02676</td>\n","      <td>0.002783</td>\n","      <td>9.456</td>\n","      <td>30.37</td>\n","      <td>59.16</td>\n","      <td>268.6</td>\n","      <td>0.08996</td>\n","      <td>0.06444</td>\n","      <td>0.0000</td>\n","      <td>0.00000</td>\n","      <td>0.2871</td>\n","      <td>0.07039</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n","559  925291         B  ...                  0.08732          NaN\n","560  925292         B  ...                  0.08321          NaN\n","561  925311         B  ...                  0.05905          NaN\n","562  925622         M  ...                  0.14090          NaN\n","563  926125         M  ...                  0.09873          NaN\n","564  926424         M  ...                  0.07115          NaN\n","565  926682         M  ...                  0.06637          NaN\n","566  926954         M  ...                  0.07820          NaN\n","567  927241         M  ...                  0.12400          NaN\n","568   92751         B  ...                  0.07039          NaN\n","\n","[10 rows x 33 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"74Tw6xk53p20"},"source":["We can see that our data set has 569 samples and 33 columns. The column `id` can be taken as an index for our pandas dataframe and `diagnosis` is the label (either **M: malignant** or **B: benign**).\n","\n","Let's prepare the data set first of all by (i) cleaning it, (ii) separating label from features, and (iii) splitting it into train and test sets."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275},"id":"2mbvvJ-Kz2Dk","executionInfo":{"status":"ok","timestamp":1612548590410,"user_tz":0,"elapsed":662,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"51171464-60b9-48ec-819b-0042d9e4b137"},"source":["# drop last column (extra column added by pd)\n","data_1 = data.drop(data.columns[-1], axis=1)\n","# set column id as dataframe index\n","data_2 = data_1.set_index(data['id']).drop(data_1.columns[0], axis=1)\n","\n","# check\n","data_2.tail()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>fractal_dimension_mean</th>\n","      <th>radius_se</th>\n","      <th>texture_se</th>\n","      <th>perimeter_se</th>\n","      <th>area_se</th>\n","      <th>smoothness_se</th>\n","      <th>compactness_se</th>\n","      <th>concavity_se</th>\n","      <th>concave points_se</th>\n","      <th>symmetry_se</th>\n","      <th>fractal_dimension_se</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>926424</th>\n","      <td>M</td>\n","      <td>21.56</td>\n","      <td>22.39</td>\n","      <td>142.00</td>\n","      <td>1479.0</td>\n","      <td>0.11100</td>\n","      <td>0.11590</td>\n","      <td>0.24390</td>\n","      <td>0.13890</td>\n","      <td>0.1726</td>\n","      <td>0.05623</td>\n","      <td>1.1760</td>\n","      <td>1.256</td>\n","      <td>7.673</td>\n","      <td>158.70</td>\n","      <td>0.010300</td>\n","      <td>0.02891</td>\n","      <td>0.05198</td>\n","      <td>0.02454</td>\n","      <td>0.01114</td>\n","      <td>0.004239</td>\n","      <td>25.450</td>\n","      <td>26.40</td>\n","      <td>166.10</td>\n","      <td>2027.0</td>\n","      <td>0.14100</td>\n","      <td>0.21130</td>\n","      <td>0.4107</td>\n","      <td>0.2216</td>\n","      <td>0.2060</td>\n","      <td>0.07115</td>\n","    </tr>\n","    <tr>\n","      <th>926682</th>\n","      <td>M</td>\n","      <td>20.13</td>\n","      <td>28.25</td>\n","      <td>131.20</td>\n","      <td>1261.0</td>\n","      <td>0.09780</td>\n","      <td>0.10340</td>\n","      <td>0.14400</td>\n","      <td>0.09791</td>\n","      <td>0.1752</td>\n","      <td>0.05533</td>\n","      <td>0.7655</td>\n","      <td>2.463</td>\n","      <td>5.203</td>\n","      <td>99.04</td>\n","      <td>0.005769</td>\n","      <td>0.02423</td>\n","      <td>0.03950</td>\n","      <td>0.01678</td>\n","      <td>0.01898</td>\n","      <td>0.002498</td>\n","      <td>23.690</td>\n","      <td>38.25</td>\n","      <td>155.00</td>\n","      <td>1731.0</td>\n","      <td>0.11660</td>\n","      <td>0.19220</td>\n","      <td>0.3215</td>\n","      <td>0.1628</td>\n","      <td>0.2572</td>\n","      <td>0.06637</td>\n","    </tr>\n","    <tr>\n","      <th>926954</th>\n","      <td>M</td>\n","      <td>16.60</td>\n","      <td>28.08</td>\n","      <td>108.30</td>\n","      <td>858.1</td>\n","      <td>0.08455</td>\n","      <td>0.10230</td>\n","      <td>0.09251</td>\n","      <td>0.05302</td>\n","      <td>0.1590</td>\n","      <td>0.05648</td>\n","      <td>0.4564</td>\n","      <td>1.075</td>\n","      <td>3.425</td>\n","      <td>48.55</td>\n","      <td>0.005903</td>\n","      <td>0.03731</td>\n","      <td>0.04730</td>\n","      <td>0.01557</td>\n","      <td>0.01318</td>\n","      <td>0.003892</td>\n","      <td>18.980</td>\n","      <td>34.12</td>\n","      <td>126.70</td>\n","      <td>1124.0</td>\n","      <td>0.11390</td>\n","      <td>0.30940</td>\n","      <td>0.3403</td>\n","      <td>0.1418</td>\n","      <td>0.2218</td>\n","      <td>0.07820</td>\n","    </tr>\n","    <tr>\n","      <th>927241</th>\n","      <td>M</td>\n","      <td>20.60</td>\n","      <td>29.33</td>\n","      <td>140.10</td>\n","      <td>1265.0</td>\n","      <td>0.11780</td>\n","      <td>0.27700</td>\n","      <td>0.35140</td>\n","      <td>0.15200</td>\n","      <td>0.2397</td>\n","      <td>0.07016</td>\n","      <td>0.7260</td>\n","      <td>1.595</td>\n","      <td>5.772</td>\n","      <td>86.22</td>\n","      <td>0.006522</td>\n","      <td>0.06158</td>\n","      <td>0.07117</td>\n","      <td>0.01664</td>\n","      <td>0.02324</td>\n","      <td>0.006185</td>\n","      <td>25.740</td>\n","      <td>39.42</td>\n","      <td>184.60</td>\n","      <td>1821.0</td>\n","      <td>0.16500</td>\n","      <td>0.86810</td>\n","      <td>0.9387</td>\n","      <td>0.2650</td>\n","      <td>0.4087</td>\n","      <td>0.12400</td>\n","    </tr>\n","    <tr>\n","      <th>92751</th>\n","      <td>B</td>\n","      <td>7.76</td>\n","      <td>24.54</td>\n","      <td>47.92</td>\n","      <td>181.0</td>\n","      <td>0.05263</td>\n","      <td>0.04362</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.1587</td>\n","      <td>0.05884</td>\n","      <td>0.3857</td>\n","      <td>1.428</td>\n","      <td>2.548</td>\n","      <td>19.15</td>\n","      <td>0.007189</td>\n","      <td>0.00466</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.02676</td>\n","      <td>0.002783</td>\n","      <td>9.456</td>\n","      <td>30.37</td>\n","      <td>59.16</td>\n","      <td>268.6</td>\n","      <td>0.08996</td>\n","      <td>0.06444</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.2871</td>\n","      <td>0.07039</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       diagnosis  radius_mean  ...  symmetry_worst  fractal_dimension_worst\n","id                             ...                                         \n","926424         M        21.56  ...          0.2060                  0.07115\n","926682         M        20.13  ...          0.2572                  0.06637\n","926954         M        16.60  ...          0.2218                  0.07820\n","927241         M        20.60  ...          0.4087                  0.12400\n","92751          B         7.76  ...          0.2871                  0.07039\n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"5zQ1tedL7dNI"},"source":["We do a bit more preparation by converting the categorical labels into 1 for **M** and -1 for **B**."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"1zbSOnY06rwL","executionInfo":{"status":"ok","timestamp":1612548591950,"user_tz":0,"elapsed":586,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"f4b1b7af-6d7c-4686-a56b-beaad44f466e"},"source":["# convert categorical labels to numbers\n","diag_map = {'M': 1.0, 'B': -1.0}\n","data_2['diagnosis'] = data_2['diagnosis'].map(diag_map)\n","\n","# put labels and features in different dataframes\n","y = data_2.loc[:, 'diagnosis']\n","X = data_2.iloc[:, 1:]\n","\n","# check\n","print(y.tail())\n","X.tail()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["id\n","926424    1.0\n","926682    1.0\n","926954    1.0\n","927241    1.0\n","92751    -1.0\n","Name: diagnosis, dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>fractal_dimension_mean</th>\n","      <th>radius_se</th>\n","      <th>texture_se</th>\n","      <th>perimeter_se</th>\n","      <th>area_se</th>\n","      <th>smoothness_se</th>\n","      <th>compactness_se</th>\n","      <th>concavity_se</th>\n","      <th>concave points_se</th>\n","      <th>symmetry_se</th>\n","      <th>fractal_dimension_se</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>926424</th>\n","      <td>21.56</td>\n","      <td>22.39</td>\n","      <td>142.00</td>\n","      <td>1479.0</td>\n","      <td>0.11100</td>\n","      <td>0.11590</td>\n","      <td>0.24390</td>\n","      <td>0.13890</td>\n","      <td>0.1726</td>\n","      <td>0.05623</td>\n","      <td>1.1760</td>\n","      <td>1.256</td>\n","      <td>7.673</td>\n","      <td>158.70</td>\n","      <td>0.010300</td>\n","      <td>0.02891</td>\n","      <td>0.05198</td>\n","      <td>0.02454</td>\n","      <td>0.01114</td>\n","      <td>0.004239</td>\n","      <td>25.450</td>\n","      <td>26.40</td>\n","      <td>166.10</td>\n","      <td>2027.0</td>\n","      <td>0.14100</td>\n","      <td>0.21130</td>\n","      <td>0.4107</td>\n","      <td>0.2216</td>\n","      <td>0.2060</td>\n","      <td>0.07115</td>\n","    </tr>\n","    <tr>\n","      <th>926682</th>\n","      <td>20.13</td>\n","      <td>28.25</td>\n","      <td>131.20</td>\n","      <td>1261.0</td>\n","      <td>0.09780</td>\n","      <td>0.10340</td>\n","      <td>0.14400</td>\n","      <td>0.09791</td>\n","      <td>0.1752</td>\n","      <td>0.05533</td>\n","      <td>0.7655</td>\n","      <td>2.463</td>\n","      <td>5.203</td>\n","      <td>99.04</td>\n","      <td>0.005769</td>\n","      <td>0.02423</td>\n","      <td>0.03950</td>\n","      <td>0.01678</td>\n","      <td>0.01898</td>\n","      <td>0.002498</td>\n","      <td>23.690</td>\n","      <td>38.25</td>\n","      <td>155.00</td>\n","      <td>1731.0</td>\n","      <td>0.11660</td>\n","      <td>0.19220</td>\n","      <td>0.3215</td>\n","      <td>0.1628</td>\n","      <td>0.2572</td>\n","      <td>0.06637</td>\n","    </tr>\n","    <tr>\n","      <th>926954</th>\n","      <td>16.60</td>\n","      <td>28.08</td>\n","      <td>108.30</td>\n","      <td>858.1</td>\n","      <td>0.08455</td>\n","      <td>0.10230</td>\n","      <td>0.09251</td>\n","      <td>0.05302</td>\n","      <td>0.1590</td>\n","      <td>0.05648</td>\n","      <td>0.4564</td>\n","      <td>1.075</td>\n","      <td>3.425</td>\n","      <td>48.55</td>\n","      <td>0.005903</td>\n","      <td>0.03731</td>\n","      <td>0.04730</td>\n","      <td>0.01557</td>\n","      <td>0.01318</td>\n","      <td>0.003892</td>\n","      <td>18.980</td>\n","      <td>34.12</td>\n","      <td>126.70</td>\n","      <td>1124.0</td>\n","      <td>0.11390</td>\n","      <td>0.30940</td>\n","      <td>0.3403</td>\n","      <td>0.1418</td>\n","      <td>0.2218</td>\n","      <td>0.07820</td>\n","    </tr>\n","    <tr>\n","      <th>927241</th>\n","      <td>20.60</td>\n","      <td>29.33</td>\n","      <td>140.10</td>\n","      <td>1265.0</td>\n","      <td>0.11780</td>\n","      <td>0.27700</td>\n","      <td>0.35140</td>\n","      <td>0.15200</td>\n","      <td>0.2397</td>\n","      <td>0.07016</td>\n","      <td>0.7260</td>\n","      <td>1.595</td>\n","      <td>5.772</td>\n","      <td>86.22</td>\n","      <td>0.006522</td>\n","      <td>0.06158</td>\n","      <td>0.07117</td>\n","      <td>0.01664</td>\n","      <td>0.02324</td>\n","      <td>0.006185</td>\n","      <td>25.740</td>\n","      <td>39.42</td>\n","      <td>184.60</td>\n","      <td>1821.0</td>\n","      <td>0.16500</td>\n","      <td>0.86810</td>\n","      <td>0.9387</td>\n","      <td>0.2650</td>\n","      <td>0.4087</td>\n","      <td>0.12400</td>\n","    </tr>\n","    <tr>\n","      <th>92751</th>\n","      <td>7.76</td>\n","      <td>24.54</td>\n","      <td>47.92</td>\n","      <td>181.0</td>\n","      <td>0.05263</td>\n","      <td>0.04362</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.1587</td>\n","      <td>0.05884</td>\n","      <td>0.3857</td>\n","      <td>1.428</td>\n","      <td>2.548</td>\n","      <td>19.15</td>\n","      <td>0.007189</td>\n","      <td>0.00466</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.02676</td>\n","      <td>0.002783</td>\n","      <td>9.456</td>\n","      <td>30.37</td>\n","      <td>59.16</td>\n","      <td>268.6</td>\n","      <td>0.08996</td>\n","      <td>0.06444</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.2871</td>\n","      <td>0.07039</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n","id                                 ...                                         \n","926424        21.56         22.39  ...          0.2060                  0.07115\n","926682        20.13         28.25  ...          0.2572                  0.06637\n","926954        16.60         28.08  ...          0.2218                  0.07820\n","927241        20.60         29.33  ...          0.4087                  0.12400\n","92751          7.76         24.54  ...          0.2871                  0.07039\n","\n","[5 rows x 30 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"PbllXkPK8fQw"},"source":["As with any data set that has features over different ranges, it's required to standardise the data before."]},{"cell_type":"code","metadata":{"id":"PSVn27p88EFl","executionInfo":{"status":"ok","timestamp":1612548593049,"user_tz":0,"elapsed":505,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["## EDIT THIS FUNCTION\n","def standardise(X):\n","  mu = np.mean(X, 0)\n","  sigma = np.std(X, 0)\n","  X_std = (X - mu) / sigma ## <-- SOLUTION\n","  return X_std"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":275},"id":"fMyxlg678EC7","executionInfo":{"status":"ok","timestamp":1612548593798,"user_tz":0,"elapsed":686,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"4e8bd490-2340-43ca-e914-5ffe918a6f5f"},"source":["X_std = standardise(X)\n","\n","# check\n","X_std.tail()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>fractal_dimension_mean</th>\n","      <th>radius_se</th>\n","      <th>texture_se</th>\n","      <th>perimeter_se</th>\n","      <th>area_se</th>\n","      <th>smoothness_se</th>\n","      <th>compactness_se</th>\n","      <th>concavity_se</th>\n","      <th>concave points_se</th>\n","      <th>symmetry_se</th>\n","      <th>fractal_dimension_se</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>926424</th>\n","      <td>2.110995</td>\n","      <td>0.721473</td>\n","      <td>2.060786</td>\n","      <td>2.343856</td>\n","      <td>1.041842</td>\n","      <td>0.219060</td>\n","      <td>1.947285</td>\n","      <td>2.320965</td>\n","      <td>-0.312589</td>\n","      <td>-0.931027</td>\n","      <td>2.782080</td>\n","      <td>0.071025</td>\n","      <td>2.379583</td>\n","      <td>2.604187</td>\n","      <td>1.086384</td>\n","      <td>0.191805</td>\n","      <td>0.666001</td>\n","      <td>2.067178</td>\n","      <td>-1.138416</td>\n","      <td>0.167980</td>\n","      <td>1.901185</td>\n","      <td>0.117700</td>\n","      <td>1.752563</td>\n","      <td>2.015301</td>\n","      <td>0.378365</td>\n","      <td>-0.273318</td>\n","      <td>0.664512</td>\n","      <td>1.629151</td>\n","      <td>-1.360158</td>\n","      <td>-0.709091</td>\n","    </tr>\n","    <tr>\n","      <th>926682</th>\n","      <td>1.704854</td>\n","      <td>2.085134</td>\n","      <td>1.615931</td>\n","      <td>1.723842</td>\n","      <td>0.102458</td>\n","      <td>-0.017833</td>\n","      <td>0.693043</td>\n","      <td>1.263669</td>\n","      <td>-0.217664</td>\n","      <td>-1.058611</td>\n","      <td>1.300499</td>\n","      <td>2.260938</td>\n","      <td>1.156857</td>\n","      <td>1.291565</td>\n","      <td>-0.424010</td>\n","      <td>-0.069758</td>\n","      <td>0.252202</td>\n","      <td>0.808431</td>\n","      <td>-0.189161</td>\n","      <td>-0.490556</td>\n","      <td>1.536720</td>\n","      <td>2.047399</td>\n","      <td>1.421940</td>\n","      <td>1.494959</td>\n","      <td>-0.691230</td>\n","      <td>-0.394820</td>\n","      <td>0.236573</td>\n","      <td>0.733827</td>\n","      <td>-0.531855</td>\n","      <td>-0.973978</td>\n","    </tr>\n","    <tr>\n","      <th>926954</th>\n","      <td>0.702284</td>\n","      <td>2.045574</td>\n","      <td>0.672676</td>\n","      <td>0.577953</td>\n","      <td>-0.840484</td>\n","      <td>-0.038680</td>\n","      <td>0.046588</td>\n","      <td>0.105777</td>\n","      <td>-0.809117</td>\n","      <td>-0.895587</td>\n","      <td>0.184892</td>\n","      <td>-0.257371</td>\n","      <td>0.276693</td>\n","      <td>0.180698</td>\n","      <td>-0.379342</td>\n","      <td>0.661277</td>\n","      <td>0.510827</td>\n","      <td>0.612157</td>\n","      <td>-0.891416</td>\n","      <td>0.036727</td>\n","      <td>0.561361</td>\n","      <td>1.374854</td>\n","      <td>0.579001</td>\n","      <td>0.427906</td>\n","      <td>-0.809587</td>\n","      <td>0.350735</td>\n","      <td>0.326767</td>\n","      <td>0.414069</td>\n","      <td>-1.104549</td>\n","      <td>-0.318409</td>\n","    </tr>\n","    <tr>\n","      <th>927241</th>\n","      <td>1.838341</td>\n","      <td>2.336457</td>\n","      <td>1.982524</td>\n","      <td>1.735218</td>\n","      <td>1.525767</td>\n","      <td>3.272144</td>\n","      <td>3.296944</td>\n","      <td>2.658866</td>\n","      <td>2.137194</td>\n","      <td>1.043695</td>\n","      <td>1.157935</td>\n","      <td>0.686088</td>\n","      <td>1.438530</td>\n","      <td>1.009503</td>\n","      <td>-0.173000</td>\n","      <td>2.017716</td>\n","      <td>1.302285</td>\n","      <td>0.785721</td>\n","      <td>0.326634</td>\n","      <td>0.904057</td>\n","      <td>1.961239</td>\n","      <td>2.237926</td>\n","      <td>2.303601</td>\n","      <td>1.653171</td>\n","      <td>1.430427</td>\n","      <td>3.904848</td>\n","      <td>3.197605</td>\n","      <td>2.289985</td>\n","      <td>1.919083</td>\n","      <td>2.219635</td>\n","    </tr>\n","    <tr>\n","      <th>92751</th>\n","      <td>-1.808401</td>\n","      <td>1.221792</td>\n","      <td>-1.814389</td>\n","      <td>-1.347789</td>\n","      <td>-3.112085</td>\n","      <td>-1.150752</td>\n","      <td>-1.114873</td>\n","      <td>-1.261820</td>\n","      <td>-0.820070</td>\n","      <td>-0.561032</td>\n","      <td>-0.070279</td>\n","      <td>0.383092</td>\n","      <td>-0.157449</td>\n","      <td>-0.466152</td>\n","      <td>0.049342</td>\n","      <td>-1.163516</td>\n","      <td>-1.057501</td>\n","      <td>-1.913447</td>\n","      <td>0.752830</td>\n","      <td>-0.382754</td>\n","      <td>-1.410893</td>\n","      <td>0.764190</td>\n","      <td>-1.432735</td>\n","      <td>-1.075813</td>\n","      <td>-1.859019</td>\n","      <td>-1.207552</td>\n","      <td>-1.305831</td>\n","      <td>-1.745063</td>\n","      <td>-0.048138</td>\n","      <td>-0.751207</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n","id                                 ...                                         \n","926424     2.110995      0.721473  ...       -1.360158                -0.709091\n","926682     1.704854      2.085134  ...       -0.531855                -0.973978\n","926954     0.702284      2.045574  ...       -1.104549                -0.318409\n","927241     1.838341      2.336457  ...        1.919083                 2.219635\n","92751     -1.808401      1.221792  ...       -0.048138                -0.751207\n","\n","[5 rows x 30 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qcw02XXM8EAe","executionInfo":{"status":"ok","timestamp":1612548593985,"user_tz":0,"elapsed":295,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"36fd1e3e-5767-4a08-eb0c-c161bba6454b"},"source":["# insert 1 in every row for intercept b\n","X_std.insert(loc=len(X_std.columns), column='intercept', value=1)\n","\n","# split into train and test set\n","# stacking data X and labels y into one matrix\n","data_split = np.hstack((X_std, y[:, np.newaxis]))\n","\n","# shuffling the rows        \n","np.random.shuffle(data_split)\n","\n","# we split train to test as 70:30\n","split_rate = 0.7\n","train, test = np.split(data_split, [int(split_rate*(data_split.shape[0]))])\n","\n","X_train = train[:,:-1]\n","y_train = train[:, -1]\n","\n","X_test = test[:,:-1]\n","y_test = test[:, -1]\n","\n","y_train = y_train.astype(float)\n","y_test = y_test.astype(float)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"jMo_v3TGCJBD"},"source":["## Linear SVM\n","We start with defining the hinge loss as\n","$$\n","\\mathcal L (\\boldsymbol w) = \\frac{1}{2} \\| \\boldsymbol w \\|^2 + \\frac{\\lambda}{n} \\sum_{i=1}^n \\max \\bigg( 0, 1-y_i (\\boldsymbol w \\cdot x_i + b) \\bigg) \\, .\n","$$\n","where $\\boldsymbol w$ is the vector of weights, $\\lambda$ the regularisation parameter, and $b$ the intercept which is included in our `X` as an additional column of $1$'s."]},{"cell_type":"code","metadata":{"id":"O8NCZ2Wj8D8m","executionInfo":{"status":"ok","timestamp":1612548597363,"user_tz":0,"elapsed":553,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["# EDIT THIS FUNCTION\n","def compute_cost(W, X, y, regul_strength=1e5):\n","  n = X.shape[0]\n","  distances = 1 - y * (np.dot(X, W))  ## <-- SOLUTION\n","  distances[distances < 0] = 0  # equivalent to max(0, distance)\n","  hinge = regul_strength * (np.sum(distances) / n)  ## <-- SOLUTION\n","\n","  # calculate cost\n","  cost = 1 / 2 * np.dot(W, W) + hinge\n","  return cost"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fPlQQlNJHgGI"},"source":["Next, we need the gradients of this cost function."]},{"cell_type":"code","metadata":{"id":"yvsu7ukAE79Y","executionInfo":{"status":"ok","timestamp":1612548598691,"user_tz":0,"elapsed":459,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["# calculate gradient of cost\n","def calculate_cost_gradient(W, X_batch, y_batch, regul_strength=1e5):\n","  # if only one example is passed\n","  if type(y_batch) == np.float64:\n","      y_batch = np.asarray([y_batch])\n","      X_batch = np.asarray([X_batch])  # gives multidimensional array\n","\n","  distance = 1 - (y_batch * np.dot(X_batch, W))\n","  dw = np.zeros(len(W))\n","\n","  for ind, d in enumerate(distance):\n","      if max(0, d)==0:\n","          di = W\n","      else:\n","          di = W - (regul_strength * y_batch[ind] * X_batch[ind])\n","      dw += di\n","\n","  dw = dw/len(y_batch)  # average\n","  return dw"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5X3_B9V-I0mX"},"source":["Both of the two previous functions are then used in SGD to update the weights iteratively with a given learning rate $\\alpha$. We also implement a stop criterion that ends the learning as soon as the cost function has not changed more than a manually determined percentage.\n","\n","We know that the learning happens through updating the weights according to\n","$$\n","\\boldsymbol w = \\boldsymbol w - \\alpha \\frac{\\partial \\mathcal L}{\\partial \\boldsymbol w}\n","$$\n","\n","where $\\frac{\\partial \\mathcal L}{\\partial \\boldsymbol w}$ is the gradient of the hinge loss we have computed in the previous cell."]},{"cell_type":"code","metadata":{"id":"a-S8N9C78D5R","executionInfo":{"status":"ok","timestamp":1612550934560,"user_tz":0,"elapsed":512,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["# EDIT THIS FUNCTION\n","def sgd(X, y, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-5, regul_strength=1e5, print_outcome=False):\n","  # initialise zero weights\n","  weights = np.zeros(X.shape[1])\n","  nth = 0\n","  # initialise starting cost as infinity\n","  prev_cost = np.inf\n","  \n","  # stochastic gradient descent\n","  for iteration in range(1, max_iterations):\n","      # shuffle to prevent repeating update cycles\n","      np.random.shuffle([X, y])\n","      for ind, x in enumerate(X):\n","          ascent = calculate_cost_gradient(weights, x, y[ind], regul_strength) ## <-- SOLUTION\n","          weights = weights - (learning_rate * ascent)\n","\n","      # convergence check on 2^n'th iteration\n","      if iteration==2**nth or iteration==max_iterations-1:\n","          # compute cost\n","          cost = compute_cost(weights, X, y, regul_strength)  ## <-- SOLUTION\n","          if print_outcome:\n","            print(\"Iteration is: {}, Cost is: {}\".format(iteration, cost))\n","          # stop criterion\n","          if abs(prev_cost - cost) < stop_criterion * prev_cost:\n","              return weights\n","          \n","          prev_cost = cost\n","          nth += 1\n","  \n","  return weights"],"execution_count":100,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTU9fyVYK8ay"},"source":["Now, we can take these functions and train a linear SVM with our training data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2bdsUvtu8D2f","executionInfo":{"status":"ok","timestamp":1612550952057,"user_tz":0,"elapsed":15455,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"9259dce8-0711-4172-f157-7a5cd4ca8569"},"source":["# train the model\n","W = sgd(X_train, y_train, max_iterations=2000, stop_criterion=0.01, learning_rate=1e-3, regul_strength=1e3, print_outcome=True)\n","print(\"Training finished.\")"],"execution_count":101,"outputs":[{"output_type":"stream","text":["Iteration is: 1, Cost is: 499.2965974291716\n","Iteration is: 2, Cost is: 638.5942711063158\n","Iteration is: 4, Cost is: 389.2082356070499\n","Iteration is: 8, Cost is: 611.3098658481113\n","Iteration is: 16, Cost is: 251.99622659053816\n","Iteration is: 32, Cost is: 301.7421196392772\n","Iteration is: 64, Cost is: 466.70496381255657\n","Iteration is: 128, Cost is: 562.309295997224\n","Iteration is: 256, Cost is: 296.892471277107\n","Iteration is: 512, Cost is: 241.00914001809392\n","Iteration is: 1024, Cost is: 533.393453002812\n","Iteration is: 1999, Cost is: 282.08448693574405\n","Training finished.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7GQqts4_O1WE"},"source":["To evaluate the mean accuracy in both train and test set, we write a small function called `score`."]},{"cell_type":"code","metadata":{"id":"5JahERSXOtJj","executionInfo":{"status":"ok","timestamp":1612548620548,"user_tz":0,"elapsed":506,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["## EDIT THIS FUNCTION\n","def score(W, X, y):\n","  y_preds = np.array([])\n","  for i in range(X.shape[0]):\n","    y_pred = np.sign(np.dot(X[i], W))\n","    y_preds = np.append(y_preds, y_pred)\n","  \n","  return np.float(sum(y_preds==y)) / float(len(y)) ## <-- SOLUTION"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oZAt83Mq8Dv6","executionInfo":{"status":"ok","timestamp":1612548622968,"user_tz":0,"elapsed":485,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"27f88db8-e716-40dc-d95c-1d7d5ddcd639"},"source":["print(\"Accuracy on train set: {}\".format(score(W, X_train, y_train)))\n","print(\"Accuracy on test set: {}\".format(score(W, X_test, y_test)))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Accuracy on train set: 0.9849246231155779\n","Accuracy on test set: 0.9415204678362573\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CePz4SbER-Qb"},"source":["#### Questions:\n","1. What are other evaluation metrices besides the accuracy? Implement them and assess the performance of our classification algorithm with them.\n","2. What makes other evaluation metrices more appropriate given our unbalanced data set _(we have more benign than malignant examples)_?\n","3. Try different learning rates, regularisation strengths and number of iterations independently. What can you observe? Can you achieve higher accuracies?\n","4. What is your understanding why have we used the hinge loss with this data set of 31 features? \n","5. Can you think of other loss functions instead of the hinge loss? What is your intuition how they will perform compared to the hinge loss? You could try implementing one and compare the results. "]},{"cell_type":"markdown","metadata":{"id":"kfmB_ih8RyUp"},"source":["## *T*-fold cross validation\n","\n","Now we repeat the same procedure as above but do not only have one train-test split, but multiple in a *T*-fold cross validation method."]},{"cell_type":"code","metadata":{"id":"3wk-Bov5K2we","executionInfo":{"status":"ok","timestamp":1612551001427,"user_tz":0,"elapsed":485,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["def cross_val_split(data, num_folds):\n","  fold_size = int(len(data) / num_folds)\n","  data_perm = np.random.permutation(data)\n","  folds = []\n","  for k in range(num_folds):\n","    folds.append(data_perm[k*fold_size:(k+1)*fold_size, :])\n","\n","  return folds"],"execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ya53BRVARxr6","executionInfo":{"status":"ok","timestamp":1612549223038,"user_tz":0,"elapsed":524,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["# evaluate\n","folds = cross_val_split(train, 5)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"tmbYTka8Rxo-","executionInfo":{"status":"ok","timestamp":1612551958072,"user_tz":0,"elapsed":1369,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}}},"source":["## EDIT THIS FUNCTION\n","def cross_val_evaluate(data, num_folds):\n","  \n","  folds = cross_val_split(data, num_folds)\n","\n","  train_scores = []\n","  val_scores = []\n","\n","  for i in range(len(folds)):\n","    print('Fold', i+1)\n","    # define the training set\n","    train_set = np.delete(np.asarray(folds).reshape(len(folds), folds[0].shape[0], folds[0].shape[1]), i, axis=0)\n","    train_folds = train_set.reshape(len(train_set)*train_set[0].shape[0], train_set[0].shape[1])\n","    X_train = train_folds[:,:-1]\n","    y_train = train_folds[:, -1]\n","    \n","    # define the validation set\n","    val_fold = folds[i]\n","    X_val = val_fold[:,:-1]\n","    y_val = val_fold[:, -1]\n","\n","    # train the model\n","    W = sgd(X_train, y_train, max_iterations=1025, stop_criterion=0.01, learning_rate=1e-3, regul_strength=1e3)\n","    print(\"Training finished.\")\n","\n","    # evaluate\n","    train_score = score(W, X_train, y_train)\n","    val_score = score(W, X_val, y_val)\n","    print(\"Accuracy on train set #{}: {}\".format(i+1, train_score))\n","    print(\"Accuracy on validation set #{}: {}\".format(i+1, val_score))\n","\n","    train_scores.append(train_score)\n","    val_scores.append(val_score)\n","\n","  return train_scores, val_scores"],"execution_count":105,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lkMF_ydbRxmJ","executionInfo":{"status":"ok","timestamp":1612551988585,"user_tz":0,"elapsed":31438,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"2c79ea1a-2946-40fc-825d-a8326845eb47"},"source":["train_scores, val_scores = cross_val_evaluate(train, 5)"],"execution_count":106,"outputs":[{"output_type":"stream","text":["Fold 1\n","Training finished.\n","Accuracy on train set #1: 0.990506329113924\n","Accuracy on validation set #1: 0.9620253164556962\n","Fold 2\n","Training finished.\n","Accuracy on train set #2: 0.990506329113924\n","Accuracy on validation set #2: 0.9746835443037974\n","Fold 3\n","Training finished.\n","Accuracy on train set #3: 0.9778481012658228\n","Accuracy on validation set #3: 1.0\n","Fold 4\n","Training finished.\n","Accuracy on train set #4: 0.9525316455696202\n","Accuracy on validation set #4: 0.9240506329113924\n","Fold 5\n","Training finished.\n","Accuracy on train set #5: 0.9841772151898734\n","Accuracy on validation set #5: 0.9873417721518988\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IukKmLiIldYx"},"source":["Finally, let's compute the mean accuracy."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Xpm2gNuTZMo","executionInfo":{"status":"ok","timestamp":1612551992361,"user_tz":0,"elapsed":883,"user":{"displayName":"Felix Laumann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmStpe6E-gLWEG78qMXTCOTw1W_IBUPBbFJXZ7pw=s64","userId":"13817614696536163905"}},"outputId":"8480d65b-a2e8-4688-9332-68e8c550907a"},"source":["print(np.mean(val_scores))"],"execution_count":107,"outputs":[{"output_type":"stream","text":["0.969620253164557\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h6wG-rfwlrB5"},"source":[""],"execution_count":null,"outputs":[]}]}