{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cardiovascular-alexander",
   "metadata": {},
   "source": [
    "# Data Science: Coursework 2\n",
    "\n",
    "### Shri Lekkala\n",
    "### CID: 01499487"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-rouge",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Task 1](#t1) \n",
    "    * [Question 1.1](#q1_1)\n",
    "        * [Part 1.1.1](#1_1_1)\n",
    "        * [Part 1.1.2](#1_1_2)\n",
    "        * [Part 1.1.3](#1_1_3)\n",
    "    * [Question 1.2](#q1_2)\n",
    "        * [Part 1.2.1](#1_2_1)\n",
    "        * [Part 1.2.2](#1_2_2)\n",
    "        * [Part 1.2.3](#1_2_3)\n",
    "        * [Part 1.2.4](#1_2_4)\n",
    "* [Task 2](#t2)\n",
    "    * [Question 2.1](#q2_1)\n",
    "        * [Part 2.1.1](#2_1_1)\n",
    "        * [Part 2.1.2](#2_1_2)\n",
    "        * [Part 2.1.3](#2_1_3)\n",
    "    * [Question 2.2](#q2_2)\n",
    "        * [Part 2.2.1](#2_2_1)\n",
    "        * [Part 2.2.2](#2_2_2)\n",
    "    * [Question 2.3](#q2_3)\n",
    "        * [Part 2.3.1](#2_3_1)\n",
    "        * [Part 2.3.2](#2_3_2)\n",
    "        * [Part 2.3.3](#2_3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules used throughout the coursework\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Set a random seed to fix the results\n",
    "np.random.seed(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-paris",
   "metadata": {},
   "source": [
    "## Task 1: Neural Networks <a class=\"anchor\" id=\"t1\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-thong",
   "metadata": {},
   "source": [
    "### 1.1 Multi-Layer Perceptron <a class=\"anchor\" id=\"q1_1\"></a>\n",
    "***\n",
    "\n",
    "#### Question 1.1.1 <a class=\"anchor\" id=\"1_1_1\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-prompt",
   "metadata": {},
   "source": [
    "First we load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "infectious-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data ():\n",
    "    (x_train, y_train), (x_val, y_val) = tf.keras.datasets.cifar10.load_data()\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_val = x_val.astype('float32') / 255\n",
    "    \n",
    "    # convert labels to categorical samples\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "    y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
    "    return ((x_train, y_train), (x_val, y_val))\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-applicant",
   "metadata": {},
   "source": [
    "We can check the shapes of each of the relevant datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quick-candle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-eligibility",
   "metadata": {},
   "source": [
    "An example image from the training dataset is plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bacterial-young",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = x_train[1024]\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-waste",
   "metadata": {},
   "source": [
    "We process the data into numpy arrays for use in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exceptional-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into 2D numpy arrays\n",
    "X_train_np = np.reshape(np.array(x_train), (50000, 3072), order='C').T\n",
    "X_test_np = np.reshape(np.array(x_val), (10000, 3072), order='C').T\n",
    "y_train_np = np.array(y_train).T\n",
    "y_test_np = np.array(y_val).T\n",
    "\n",
    "# Convert the vector of labels of 0s and 1s into an integer between 0 and 9\n",
    "y_train_np_labels = np.argmax(y_train_np, axis = 0)\n",
    "y_test_np_labels = np.argmax(y_test_np, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-custom",
   "metadata": {},
   "source": [
    "Once again we can check their shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reasonable-inclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-insight",
   "metadata": {},
   "source": [
    "First we create a function to randomly split the training dataset into batches of 128.\n",
    "\n",
    "50,000 datapoints means that there will be 391 batches in total.\\\n",
    "Note here we make the final batch also size 128 by including some datapoints from the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "enclosed-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def databatch(batch_size, x_data, y_data):\n",
    "    n = x_data.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    \n",
    "    # shuffle before each epoch\n",
    "    np.random.shuffle(indices)\n",
    "    x_data = x_data[indices]\n",
    "    y_data = y_data[indices]\n",
    "    \n",
    "    batches = [(x_data[k:k+batch_size], y_data[k:k+batch_size]) for k in range(0, n, batch_size)]\n",
    "    \n",
    "    # make the last batch the same size as the others by including elements from the start\n",
    "    k = range(0, 50000, 128)[-1]\n",
    "    last_batch_x = np.array([x_data[i%n] for i in range(k, k+batch_size)])\n",
    "    last_batch_y = np.array([y_data[i%n] for i in range(k, k+batch_size)])\n",
    "    \n",
    "    batches[-1] = (last_batch_x, last_batch_y)\n",
    "    \n",
    "    return batches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-terminal",
   "metadata": {},
   "source": [
    "Next we create activation functions for the forward pass in the MLP, and also the derivative of the tanh activation dunction for use in backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "otherwise-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tanh(h):\n",
    "    return np.tanh(h)\n",
    "\n",
    "def SoftMax(X):\n",
    "    prob = np.exp(X) /np.sum(np.exp(X), axis=0)\n",
    "    return prob\n",
    "\n",
    "def dTanh(a1):\n",
    "    # Note the derivative of tanh(x) is 1 - tanh^2(x)\n",
    "    return 1 - np.square(np.tanh(a1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-union",
   "metadata": {},
   "source": [
    "We also have a function to compute the Cross Entropy Loss, using the formula \n",
    "\n",
    "$$ CE = - \\sum t_i \\log(f(s)_i) $$ where $\\mathbf{t}$ is the one-hot truth vector, and $\\mathbf{f(s)}$ is the output from the SoftMax activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "registered-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(y_truth, y_hat_softmax):\n",
    "    ce_loss = - np.sum(y_truth * np.log(y_hat_softmax)) / y_truth.shape[1]\n",
    "    return ce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-ceiling",
   "metadata": {},
   "source": [
    "We also create a function for the dense layers to obtain the pre-activation values, and also a function to compute the output_error for use in back propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "falling-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(h, W, b):\n",
    "    return b + W @ h\n",
    "\n",
    "def output_error(y_batch, a6):\n",
    "    return a6 - y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-policy",
   "metadata": {},
   "source": [
    "Throughout this section, I will be using the same notation as defined in the lectures:\n",
    "\n",
    "Pre-activations: $\\mathbf{a}^{(k)} + \\mathbf{W}^{(k-1)} \\mathbf{h}^{(k-1)} + \\mathbf{b}^{(k-1)} $ \\\n",
    "Post-activations: $\\mathbf{h}^{(k)} = \\sigma(\\mathbf{a}^{(k)}) $\n",
    "\n",
    "In the code, I store all the relevant parameters in python dictionaries for easy access.\\\n",
    "In particular the pre and post activations of each layer are stored in the dictionary 'activations'\\\n",
    "The weights and biases of the layers are stored in the dictionary 'parameters'\\\n",
    "The relevant gradients for each layer are stored in the dictionary 'gradient'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-indication",
   "metadata": {},
   "source": [
    "First we create the function to compute the forward pass, and keep a record of all the pre and post activations in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adverse-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, parameters):\n",
    "    # create a dictionary to store the pre and post activations\n",
    "    activations = {}\n",
    "    \n",
    "    # Five Hidden Layers\n",
    "    activations['a1'] = dense(X, parameters['W0'], parameters['b0'])\n",
    "    activations['h1'] = Tanh(activations['a1'])\n",
    "    \n",
    "    activations['a2'] = dense(activations['h1'], parameters['W1'], parameters['b1'])\n",
    "    activations['h2'] = Tanh(activations['a2'])\n",
    "    \n",
    "    activations['a3'] = dense(activations['h2'], parameters['W2'], parameters['b2'])\n",
    "    activations['h3'] = Tanh(activations['a3'])\n",
    "    \n",
    "    activations['a4'] = dense(activations['h3'], parameters['W3'], parameters['b3'])\n",
    "    activations['h4'] = Tanh(activations['a4'])\n",
    "    \n",
    "    activations['a5'] = dense(activations['h4'], parameters['W4'], parameters['b4'])\n",
    "    activations['h5'] = Tanh(activations['a5'])\n",
    "    \n",
    "    # Output Layer\n",
    "    activations['a6'] = dense(activations['h5'], parameters['W5'], parameters['b5'])\n",
    "    activations['h6'] = SoftMax(activations['a6'])\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-doctor",
   "metadata": {},
   "source": [
    "Next, we have a function to compute all the required gradients by back propagation using the pre and post activations obtained from the function above.\n",
    "\n",
    "We use the rules derived from the lectures:\n",
    "\n",
    "$\\Large{\\boldsymbol{\\delta_p}^{(k)} := \\frac{\\partial\\mathcal{L}_i}{\\partial \\boldsymbol{a_p}^{(k)}}}$\n",
    "\n",
    "$\\Large{\\frac{\\partial\\mathcal{L}_i}{\\partial \\boldsymbol{W_{pq}}^{(k)}} = \\frac{\\partial\\mathcal{L}_i}{\\partial \\boldsymbol{a_p}^{(k+1)}} \\cdot h_q^{k}}$\n",
    "\n",
    "$\\Large{\\frac{\\partial\\mathcal{L}_i}{\\partial \\boldsymbol{b_p}^{(k)}} = \\frac{\\partial\\mathcal{L}_i}{\\partial \\boldsymbol{a_p}^{(k+1)}}}$\n",
    "\n",
    "\n",
    "Note that we also sum the vector of gradients and divide by the number of datapoints in the batch to get an average value over the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "invalid-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagate(X, y, activations, parameters):\n",
    "    D = X.shape[1]\n",
    "    \n",
    "    # create a dictionary to store the gradients\n",
    "    gradient = {}\n",
    "    \n",
    "    gradient['delta6'] = output_error(y, activations['h6'])\n",
    "    gradient['d_W5'] = gradient['delta6'] @ activations['h5'].T / D\n",
    "    gradient['d_b5'] = np.sum(gradient['delta6'], axis=1, keepdims=True) / D\n",
    "    gradient['d_h5'] = parameters['W5'].T @ gradient['delta6']\n",
    "    \n",
    "    gradient['delta5'] = gradient['d_h5'] * dTanh(activations['a5'])\n",
    "    gradient['d_W4'] = gradient['delta5'] @ activations['h4'].T / D\n",
    "    gradient['d_b4'] = np.sum(gradient['delta5'], axis=1, keepdims=True) / D\n",
    "    gradient['d_h4'] = parameters['W4'].T @ gradient['delta5']\n",
    "    \n",
    "    gradient['delta4'] = gradient['d_h4'] * dTanh(activations['a4'])\n",
    "    gradient['d_W3'] = gradient['delta4'] @ activations['h3'].T / D\n",
    "    gradient['d_b3'] = np.sum(gradient['delta4'], axis=1, keepdims=True) / D\n",
    "    gradient['d_h3'] = parameters['W3'].T @ gradient['delta4']\n",
    "    \n",
    "    gradient['delta3'] = gradient['d_h3'] * dTanh(activations['a3'])\n",
    "    gradient['d_W2'] = gradient['delta3'] @ activations['h2'].T / D\n",
    "    gradient['d_b2'] = np.sum(gradient['delta3'], axis=1, keepdims=True) / D\n",
    "    gradient['d_h2'] = parameters['W2'].T @ gradient['delta3']\n",
    "    \n",
    "    gradient['delta2'] = gradient['d_h2'] * dTanh(activations['a2'])\n",
    "    gradient['d_W1'] = gradient['delta2'] @ activations['h1'].T / D\n",
    "    gradient['d_b1'] = np.sum(gradient['delta2'], axis=1, keepdims=True) / D\n",
    "    gradient['d_h1'] = parameters['W1'].T @ gradient['delta2']\n",
    "    \n",
    "    gradient['delta1'] = gradient['d_h1'] * dTanh(activations['a1'])\n",
    "    gradient['d_W0'] = gradient['delta1'] @ X.T / D\n",
    "    gradient['d_b0'] = np.sum(gradient['delta1']) / D\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-diameter",
   "metadata": {},
   "source": [
    "Once we have the all the gradient terms, we can then use stochastic gradient descent to update our parameter values of weights and biases in the direction of the negative gradient with a specified learning rate.\n",
    "\n",
    "For a general parameter $\\theta$, learning rate $\\eta$, loss function $L$, mini-batch $\\mathcal{D}_m$ and iteration $t$ the update rule is: $\\theta_{t+1} = \\theta_t - \\eta \\Delta_{\\theta} L (\\theta_t; \\mathcal{D}_m)$\n",
    "\n",
    "This function computes these new parameter values and updates the values in the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "australian-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_optimize(parameters, gradient, learning_rate):\n",
    "    parameters['W5'] -= learning_rate * gradient['d_W5']\n",
    "    parameters['b5'] -= learning_rate * gradient['d_b5']\n",
    "    \n",
    "    parameters['W4'] -= learning_rate * gradient['d_W4']\n",
    "    parameters['b4'] -= learning_rate * gradient['d_b4']\n",
    "\n",
    "    parameters['W3'] -= learning_rate * gradient['d_W3']\n",
    "    parameters['b3'] -= learning_rate * gradient['d_b3']\n",
    "    \n",
    "    parameters['W2'] -= learning_rate * gradient['d_W2']\n",
    "    parameters['b2'] -= learning_rate * gradient['d_b2']\n",
    "    \n",
    "    parameters['W1'] -= learning_rate * gradient['d_W1']\n",
    "    parameters['b1'] -= learning_rate * gradient['d_b1']\n",
    "    \n",
    "    parameters['W0'] -= learning_rate * gradient['d_W0']\n",
    "    parameters['b0'] -= learning_rate * gradient['d_b0']\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-spoke",
   "metadata": {},
   "source": [
    "Finally we have a function that classifies a set of data points $X$ given $X$ and the parameters (weights and biases) of the MLP model by passing through all the dense and activation layers.\n",
    "\n",
    "The output from the SoftMax layer, h6, is converted into a vector of labels by using argmax. \\\n",
    "However we return both the predictions and h6 in order to compute accuracies and losses respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "martial-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(X, parameters):\n",
    "    a1 = dense(X, parameters['W0'], parameters['b0'])\n",
    "    h1 = Tanh(a1)\n",
    "    \n",
    "    a2 = dense(h1, parameters['W1'], parameters['b1'])\n",
    "    h2 = Tanh(a2)\n",
    "    \n",
    "    a3 = dense(h2, parameters['W2'], parameters['b2'])\n",
    "    h3 = Tanh(a3)\n",
    "    \n",
    "    a4 = dense(h3, parameters['W3'], parameters['b3'])\n",
    "    h4 = Tanh(a4)\n",
    "    \n",
    "    a5 = dense(h4, parameters['W4'], parameters['b4'])\n",
    "    h5 = Tanh(a5)\n",
    "    \n",
    "    a6 = dense(h5, parameters['W5'], parameters['b5'])\n",
    "    h6 = SoftMax(a6)\n",
    "    \n",
    "    preds = np.argmax(h6, axis=0)\n",
    "    return h6, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-memorabilia",
   "metadata": {},
   "source": [
    "Next we have a function to randomly intialise a set of weights to be used at the beginning of model training. The biases are intialised as zero vectors.\n",
    "\n",
    "The dimensions of the arrays for each parameter depends on the number of neurons in the previous layer and the number of neurons in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "facial-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_parameters(num_h, D):\n",
    "    # num_h: Number of Neurons in hidden layer\n",
    "    # D: Number of neurons in layer 0 (Number of inputs)\n",
    "    # The number of neurons in the last layer (output layer) is fixed at 10\n",
    "    parameters = {'W0': np.random.randn(num_h, D) * 0.05,\n",
    "                  'b0': np.zeros((num_h, 1)),\n",
    "                  \n",
    "                  'W1': np.random.randn(num_h, num_h) * 0.05,\n",
    "                  'b1': np.zeros((num_h, 1)),\n",
    "                    \n",
    "                  'W2': np.random.randn(num_h, num_h) * 0.05,\n",
    "                  'b2': np.zeros((num_h, 1)),\n",
    "                     \n",
    "                  'W3': np.random.randn(num_h, num_h) * 0.05,\n",
    "                  'b3': np.zeros((num_h, 1)),\n",
    "                     \n",
    "                  'W4': np.random.randn(num_h, num_h) * 0.05,\n",
    "                  'b4': np.zeros((num_h, 1)),\n",
    "                     \n",
    "                  'W5': np.random.randn(10, num_h) * 0.05,\n",
    "                  'b5': np.zeros((10, 1))}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-color",
   "metadata": {},
   "source": [
    "Finally we can create an overall function to train the MLP as required by using all the functions above.\n",
    "\n",
    "The inputs are the hyperparameters: number of epochs and learning rate.\\\n",
    "The outputs are dictionaries containing the metrics for this model: training & validation losses, training & validation accuracies, total training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "funny-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(num_epochs, l_rate):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Create lists to store accuracies and losses over the epochs\n",
    "    Accuracies = []\n",
    "    Losses = []\n",
    "\n",
    "    # Set the number of neurons per hidden layer\n",
    "    num_h = 400\n",
    "    \n",
    "    # Get the initialised set of parameters\n",
    "    Params = initial_parameters(num_h, X_train_np.shape[0])\n",
    "    \n",
    "    ## Loop over the epochs\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Obtain new randomly sampled batches for each epoch\n",
    "        # (This will return 391 batches of 128 to cover all the 50,000 data points)\n",
    "        batches = databatch(128, X_train_np.T, y_train_np.T)\n",
    "        \n",
    "        # Loop over each batch\n",
    "        for x_batch, y_batch in batches:\n",
    "            X = x_batch.T\n",
    "            y = y_batch.T\n",
    "        \n",
    "            # Compute all the pre and post activations in the Forward Pass\n",
    "            forwardPass = forward_pass(X, Params)\n",
    "            \n",
    "            # Back Propagate to find the gradients\n",
    "            gradient = back_propagate(X, y, forwardPass, Params)\n",
    "        \n",
    "            # Update the parameters according to Stochastic Gradient Descent\n",
    "            Params = SGD_optimize(Params, gradient, l_rate)\n",
    "        \n",
    "        # Obtain the predictions and softmax outputs using the model for the training and validation sets\n",
    "        y_train_softmax, y_train_hat = get_preds(X_train_np, Params)\n",
    "        y_val_softmax, y_val_hat = get_preds(X_test_np, Params)\n",
    "        \n",
    "        # Compute the percentage accuracies\n",
    "        train_acc = np.sum(y_train_hat == y_train_np_labels) / len(y_train_np_labels)\n",
    "        val_acc = np.sum(y_val_hat == y_test_np_labels) / len(y_test_np_labels)\n",
    "        \n",
    "        # Compute the losses\n",
    "        train_loss = CrossEntropyLoss(y_train_np, y_train_softmax)\n",
    "        val_loss = CrossEntropyLoss(y_test_np, y_val_softmax)\n",
    "        \n",
    "        # Store the the metrics in the lists\n",
    "        Losses.append((train_loss, val_loss))\n",
    "        Accuracies.append(((train_acc, val_acc)))\n",
    "        \n",
    "        # Print the progress\n",
    "        if i%5 == 4:\n",
    "            print(\"Epoch \" + str(i+1) + \"/ 40\")\n",
    "    \n",
    "    # Calculate the time taken\n",
    "    time_diff = time.time() - start\n",
    "    train_time = time.strftime(\"%H:%M:%S\", time.gmtime(time_diff))\n",
    "    \n",
    "    # Return the metrics\n",
    "    return Losses, Accuracies, train_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-resident",
   "metadata": {},
   "source": [
    "As required we train this MLP on batches of 128 datapoints with a learning rate of 0.01 for 40 epochs.\\\n",
    "The metrics obtained are stored in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-ocean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/ 40\n"
     ]
    }
   ],
   "source": [
    "MLP_Metrics = {}\n",
    "MLP_Metrics[\"40, 0.01\"] = MLP(40, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Validation Loss           :\", MLP_Metrics[\"40, 0.01\"][0][-1][1])\n",
    "print('Final Validation Accuracy       :', MLP_Metrics[\"40, 0.01\"][1][-1][1])\n",
    "print('Total Training Time  : ', MLP_Metrics[\"40, 0.01\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-selection",
   "metadata": {},
   "source": [
    "We then create a function to create plots from the metrics we obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_plots(key, lrate):\n",
    "    # Obtain the metrics\n",
    "    train_loss = np.array(MLP_Metrics[key][0])[:, 0]\n",
    "    val_loss = np.array(MLP_Metrics[key][0])[:, 1]\n",
    "    train_acc = np.array(MLP_Metrics[key][1])[:, 0]\n",
    "    val_acc = np.array(MLP_Metrics[key][1])[:, 1]\n",
    "    \n",
    "    # Plot the metrics\n",
    "    x_axis = np.arange(40) + 1\n",
    "    plt.title(\"Plot of Cross-Entropy losses over Number of Epochs, [Learning Rate = \" + lrate + \"]\")\n",
    "    plt.plot(x_axis, train_loss, label = \"Training Set\")\n",
    "    plt.plot(x_axis, val_loss, label = \"Validation Set\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"Plot of Accuracies over Number of Epochs [Learning Rate = \" + lrate + \"]\")\n",
    "    plt.plot(x_axis, train_acc, label = \"Training Set\")\n",
    "    plt.plot(x_axis, val_acc, label = \"Validation Set\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Epoch Number\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-binding",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_plots(\"40, 0.01\", \"0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-delivery",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vocational-vacuum",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "charged-poster",
   "metadata": {},
   "source": [
    "#### Question 1.1.2 <a class=\"anchor\" id=\"1_1_2\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-contrary",
   "metadata": {},
   "source": [
    "We use the same functions above to train the MLP with different learning rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Metrics[\"40, 0.0001\"] = MLP(40, 0.0001)\n",
    "print('Total Training Time  : ', MLP_Metrics[\"40, 0.0001\"][2])\n",
    "\n",
    "MLP_Metrics[\"40, 0.1\"] = MLP(40, 0.1)\n",
    "print('Total Training Time  : ', MLP_Metrics[\"40, 0.1\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "The plots obtained are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_plots(\"40, 0.0001\", \"0.0001\")\n",
    "MLP_plots(\"40, 0.1\", \"0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-europe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-tutorial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-block",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "affecting-teens",
   "metadata": {},
   "source": [
    "#### Question 1.1.3 <a class=\"anchor\" id=\"1_1_3\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-pickup",
   "metadata": {},
   "source": [
    "Finally we train the MLP over 80 epochs with the learning rate 0.01 and again plot the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Metrics[\"80, 0.01\"] = MLP(80, 0.01)\n",
    "print('Total Training Time  : ', MLP_Metrics[\"80, 0.01\"][2])\n",
    "\n",
    "MLP_plots(\"80, 0.01\", \"0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-aruba",
   "metadata": {},
   "source": [
    "### 1.2 Convolutional Neural Network (CNN) <a class=\"anchor\" id=\"q1_2\"></a>\n",
    "***\n",
    "\n",
    "#### Question 1.2.1 <a class=\"anchor\" id=\"1_2_1\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-folks",
   "metadata": {},
   "source": [
    "#### Question 1.2.2 <a class=\"anchor\" id=\"1_2_2\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-statistics",
   "metadata": {},
   "source": [
    "#### Question 1.2.3 <a class=\"anchor\" id=\"1_2_3\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-label",
   "metadata": {},
   "source": [
    "#### Question 1.2.4 <a class=\"anchor\" id=\"1_2_4\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-procedure",
   "metadata": {},
   "source": [
    "## Task 2: Unsupervised Learning <a class=\"anchor\" id=\"t2\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-teach",
   "metadata": {},
   "source": [
    "### 2.1 Clustering of the Feature Matrix <a class=\"anchor\" id=\"q2_1\"></a>\n",
    "***\n",
    "\n",
    "#### Question 2.1.1 <a class=\"anchor\" id=\"2_1_1\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-thailand",
   "metadata": {},
   "source": [
    "#### Question 2.1.2 <a class=\"anchor\" id=\"2_1_2\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-advertiser",
   "metadata": {},
   "source": [
    "#### Question 2.1.3 <a class=\"anchor\" id=\"2_1_3\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-recognition",
   "metadata": {},
   "source": [
    "### 2.2 Dimensionality Reduction of the Feature Matrix <a class=\"anchor\" id=\"q2_2\"></a>\n",
    "***\n",
    "\n",
    "#### Question 2.2.1 <a class=\"anchor\" id=\"2_2_1\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-hughes",
   "metadata": {},
   "source": [
    "#### Question 2.2.2 <a class=\"anchor\" id=\"2_2_2\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-literacy",
   "metadata": {},
   "source": [
    "### 2.3 Graph-Based Analysis <a class=\"anchor\" id=\"q2_3\"></a>\n",
    "***\n",
    "\n",
    "#### Question 2.3.1 <a class=\"anchor\" id=\"2_3_1\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-today",
   "metadata": {},
   "source": [
    "#### Question 2.3.2 <a class=\"anchor\" id=\"2_3_2\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-wrist",
   "metadata": {},
   "source": [
    "#### Question 2.3.3 <a class=\"anchor\" id=\"2_3_3\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-implementation",
   "metadata": {},
   "source": [
    "## End of Coursework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
