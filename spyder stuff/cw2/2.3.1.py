# importsimport numpy as npimport matplotlib.pyplot as pltimport matplotlib.colors as mpl_colorsimport csvimport networkx as nxfrom scipy.sparse import linalgimport pandas as pdimport seaborn as snsimport sklearn.metricsnp.random.seed(1024)# Load the matrices# Feature MatrixF = np.loadtxt(open("data/feature_matrix_karate_club.csv", "rb"), delimiter=",", skiprows=1)individuals = F[:, 0]feature_matrix = F[:, 1:]A = np.loadtxt(open("data/karate_club_graph.csv", "rb"), delimiter=",", skiprows=1)adjacency_matrix = A[:, 1:]S = list(csv.reader(open("data/ground_truth_karate_club.csv")))true_split = S[1:]# Convert to numpy array with "Mr Hi" being 0 and "Officer" being 1true_split_np = np.vstack((np.arange(34), np.zeros(34))).Tfor i in range(34):    if true_split[i][1] == "Officer":        true_split_np[i, 1] = 1              """Network X"""      network_g = nx.karate_club_graph()# extracting adjacency matrixnetwork_A = nx.adjacency_matrix(network_g).toarray().astype(np.float64)# build graph from CSV adjacency matrixG = nx.from_numpy_array(adjacency_matrix)G.nodesG.edgesG.number_of_nodes()G.number_of_edges()nx.draw(G)plt.show()"""2.3.1"""def degree_cent(A):    # compute degrees from the adjacency matrix    degree = A.sum(axis=1)        # compute the total number of edges    e = np.sum(A) / 2        # divide degree by 2E to get the centrality    return degree / (2 * e)    def pagerank_cent(A):    # compute the inverse of the degree matrix    Dinv = np.diag(np.reciprocal(A.sum(axis=1)))        # number of nodes    N = A.shape[0]        # set the teleportation parameter to the customary value    alpha = 0.85            # randomly initialise the vector c_pr    c_pr = np.random.randn(N)        # compute the page rank centrality via the power iteration method        max_iterations = 100    for i in range(max_iterations):        old_c_pr = c_pr        c_pr = alpha * A @ Dinv @ old_c_pr + (1 - alpha) / N                # check for convergence        if np.linalg.norm(c_pr - old_c_pr) < 1e-9:            break        return c_prdef eigenvector_cent(A):    # compute the eigenvector associated with the largest eigenvalue    eigenvalue, eigenvector = linalg.eigsh(A, 1, which="LM", return_eigenvectors=True)        # sometimes scipy returns negative eigenvector instead, so change sign accordingly    if eigenvector[0][0] < 0:        eigenvector = -1 * eigenvector    return eigenvector.T[0]# compute the three centralitiesc_pagerank = pagerank_cent(adjacency_matrix)c_degree = degree_cent(adjacency_matrix)c_eigenvec = eigenvector_cent(adjacency_matrix)"""# compute the centralities via NetworkXc_pr = np.array(list(nx.pagerank(network_g, alpha=0.85).values()))degree_centrality = np.array(list(nx.centrality.degree_centrality(network_g).values()))c_d = degree_centrality / np.sum(degree_centrality)c_e = np.array(list(nx.eigenvector_centrality(network_g).values()))# check they are all the sameprint(np.linalg.norm(c_pagerank - c_pr))print(np.linalg.norm(c_degree - c_d))print(np.linalg.norm(c_eigenvec - c_e))"""# Report the values of the centralitiescentralities_comparison = pd.DataFrame(index = ["Node " + str(i) for i in range(1, 35)],                                       columns = ["PageRank", "Degree", "Eigenvector"],                                       data = np.vstack((c_pagerank, c_degree, c_eigenvec)).T)centralities_comparison# Plot the standardised centralities (so that their sum is 1)# Note sum of c_degree and c_pagerank is already oneplt.plot(range(1, 35), c_degree, 'rx', label = "Degree")plt.plot(range(1, 35), c_pagerank, 'bx', label = "PageRank")plt.plot(range(1, 35), c_eigenvec / np.sum(c_eigenvec), 'gx', label = "Eigenvector")plt.xlabel("Node Number")plt.ylabel("Standardised Centrality Measures")plt.legend()plt.title("Centralities of all the Nodes of the Feature Matrix")plt.show()# Rank in decreasing orderpagerank_ranking = np.argsort(-1 * c_pagerank)degree_ranking = np.argsort(-1 * c_degree)eigenvec_ranking = np.argsort(-1 * c_eigenvec)np.vstack((pagerank_ranking, degree_ranking, eigenvec_ranking))sorted_centralities = pd.DataFrame(index = ["PageRank", "Degree", "Eigenvector"],                                   columns = ["Highest Centrality"] + [" " for i in range(32)] + ["Lowest Centrality"],                                   data=np.vstack((pagerank_ranking,                                                    degree_ranking,                                                    eigenvec_ranking)))sorted_centralities# 33, 0, 32, 3, 1 appears in top 5 of all three centralities# 11 appears in bottom of all 3 centralities# 16, 22, 26 appear in bottom 5 of two of the three centralities# Correlation Plotsprint(centralities_comparison.corr())sns.heatmap(centralities_comparison.corr(), annot=True, fmt='.3f')plt.suptitle("Heatmap of the Correlation Matrix for Centrality Measures")plt.show()"""Using NetworkX valuesdf_nx_graph_centrality = pd.DataFrame(np.array([c_pr, c_d, c_e]).T,                                       columns=["PageRank Nx", "Degree Nx", "Eigenvector Nx"])sns.heatmap(df_nx_graph_centrality.corr(), annot=True, fmt='.3f')plt.show()"""# Pair Plotsfig = sns.pairplot(centralities_comparison, kind='reg')plt.suptitle("Pair Plots comparing Centrality Measures",fontsize=15, y=1.05)plt.show()# centrality ranking discussion as in CW3"""2.3.2"""# build graph from CSV adjacency matrixG = nx.from_numpy_array(adjacency_matrix)G.nodesG.edgesG.number_of_nodes()G.number_of_edges()nx.draw(G)plt.show()communities = list(nx.algorithms.community.greedy_modularity_communities(G))communities_nx = list(nx.algorithms.community.greedy_modularity_communities(network_g))optimal_k_star = len(communities)partition = [sorted(communities[i]) for i in range(3)]print("The optimal number of communities is: " + str(optimal_k_star))for i in range(optimal_k_star):    print("Nodes in Community " + str(i+1) + " are: " + str(partition[i]))# Use NetworkX to plot the obtained clusterspos = nx.spring_layout(G)nx.draw(G,pos)node_community_id = []for label in list(G.nodes()):    for k in range(optimal_k_star):        if label in partition[k]:            col = k            node_community_id.append(col)            break        # adjust colors for visibilitycmap = plt.cm.viridis(np.linspace(0,1,3))cmap[-1, :] = np.array([255/256, 153/256, 0, 1])cmap = mpl_colors.ListedColormap(cmap[:, :-1])# draw graphnx.draw(G, pos, node_color=node_community_id, with_labels = True, cmap=cmap, font_color='w')plt.show()# Distribution of top 8 most central nodes across k^* communitiestop_8_degree = degree_ranking[:8]top_8_pagerank = pagerank_ranking[:8]degree_dist = []pagerank_dist = []for i in range(3):    degree_dist.append([node for node in degree_ranking[:8] if node in partition[i]])    pagerank_dist.append([node for node in pagerank_ranking[:8] if node in partition[i]])# Bar Graphplt.bar(np.arange(1, 4) - 0.2, [len(degree_dist[i]) for i in range(3)], 0.4, label = "Degree")plt.bar(np.arange(1, 4) + 0.2, [len(pagerank_dist[i]) for i in range(3)], label = "Page Rank", width=0.4)plt.xlabel("Community")plt.ylabel("Number of Nodes")plt.xticks(np.arange(1, 4))plt.yticks(np.arange(1, 5))plt.legend()plt.title("Distribution of Top 8 most central nodes across the $k^*$ communities")plt.show()# Line Graphplt.plot(np.arange(1, 4), [len(degree_dist[i]) for i in range(3)], label = "Degree")plt.plot(np.arange(1, 4), [len(pagerank_dist[i]) for i in range(3)], label = "Page Rank")plt.xlabel("Community")plt.ylabel("Number of Nodes")plt.xticks(np.arange(1, 4))plt.yticks(np.arange(1, 5))plt.legend()plt.title("Distribution of Top 8 most central nodes across the $k^*$ communities")plt.show()# Plot changing node size graphsfig, axes = plt.subplots(1, 2, figsize=(10, 4))ax = axes.flatten()# Highlist Most Central Nodes according to Degreenx.draw(G, pos, node_size = c_degree*10000, node_color=node_community_id,        cmap=cmap, font_color='w', ax=ax[0])ax[0].title.set_text('Degree Centrality')# Highlist Most Central Nodes According to PageRanknx.draw(G, pos, node_size = c_pagerank*10000, node_color=node_community_id,        cmap=cmap, font_color='w', ax=ax[1])ax[1].title.set_text('PageRank Centrality')plt.suptitle('Network Graphs with Node Size according to Centrality Measures', y=1.05, fontsize=15)plt.show()"""2.3.3"""## Get the 3 clustering lists# k-Meansoptimal_labels_1 = np.array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,                          1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1])# modularity maximisationoptimal_labels_2 = np.array(node_community_id)# ground truthground_truth = true_split_np[:, 1]## Code the ARI# Create contingency tabledef contingency_table(X, Y):        # Find the number of clusters in each clustering array    r = len(np.unique(X))    s = len(np.unique(Y))    contingency_mat = np.zeros((r, s))    for i in range(r):        # Cluster i of X        X_i = np.where(X == i)[0]                for j in range(s):            # Cluster j of Y            Y_j = np.where(Y == j)[0]                        # Find the number of common elements in each cluster            contingency_mat[i, j] = len(np.intersect1d(X_i, Y_j))                return contingency_mat## N choose 2 function just in casedef nChoose2(n):    if n < 2:        return 0    numerator = np.math.factorial(n)    denominator = np.math.factorial(n-2) * 2    return numerator / denominatordef ARI_score(labels_1, labels_2):    # Get the contingency table    C_T = contingency_table(labels_1, labels_2)        r, s = C_T.shape        # Compute row, column and total sums    a = np.sum(C_T, axis = 1)    b = np.sum(C_T, axis = 0)    n = np.sum(a)        # Initialise variables    sum_n_ij = 0    sum_a_i = 0    sum_b_j = 0        # Compute required quantities by looping over the rows and columns of the contingency table    for i in range(r):        for j in range(s):            sum_n_ij += nChoose2(C_T[i, j])        for i in range(r):        sum_a_i += nChoose2(a[i])            for j in range(s):        sum_b_j += nChoose2(b[j])        # Compute the ARI and return    numerator = sum_n_ij - (sum_a_i * sum_b_j) / nChoose2(n)    denominator = 0.5 * (sum_a_i + sum_b_j) - (sum_a_i * sum_b_j) / nChoose2(n)    ARI = numerator / denominator    return ARIARI_scores = np.array([ARI_score(optimal_labels_1, optimal_labels_2),              ARI_score(optimal_labels_1, ground_truth),              ARI_score(optimal_labels_2, ground_truth)])ARI_sklearn = [sklearn.metrics.adjusted_rand_score(optimal_labels_1, optimal_labels_2),               sklearn.metrics.adjusted_rand_score(optimal_labels_1, ground_truth),               sklearn.metrics.adjusted_rand_score(optimal_labels_2, ground_truth)]## "Though the Rand Index may only yield a value between 0 and +1, ## the adjusted Rand index can yield negative values if the index ## is less than the expected index."ARI_comparison = pd.DataFrame(ARI_scores,                              columns = ["ARI score"],                              index = ["Optimal k-Means vs Optimal Modularity Maximisation",                                         "Optimal k-Means vs Ground Truth",                                         "Optimal Modularity Maximisation vs Ground Truth"])ARI_comparison